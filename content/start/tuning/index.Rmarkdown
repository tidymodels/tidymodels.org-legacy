---
title: "Tune model parameters"
weight: 4
tags: [rsample, parsnip, tune, dials, yardstick]
categories: [tuning]
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
source(here::here("content/start/common.R"))
```

```{r load, include = FALSE, message = FALSE, warning = FALSE}
library(tidymodels)
library(ranger)
library(modeldata)
library(kableExtra)
theme_set(theme_bw())
doParallel::registerDoParallel()
```

Some model parameters cannot be learned directly from a data set during model training; these kinds of parameters are called **hyperparameters**. Some examples of hyperparameters include the number of predictors that are sampled at splits in a tree-based model (we call this `mtry` in tidymodels) or the learning rate in a boosted tree model (we call this `learn_rate`). Instead of learning these kinds of hyperparameters during model training, we can estimate the best values for these values by training many models on resampled data sets and exploring how well all these models perform. This process is called **tuning**.

## Predicting image segmentation, but better

In our [article on resampling](LINKTODO), we introduced a data set of images of cells that were labeled by experts as well-segmented (`WS`) or poorly segmented (`PS`). We trained a random forest model to predict which images are segmented well vs. poorly, so that a biologist could filter out poorly segmented cell images in their analysis. We used resampling to estimate the performance of our model on this data.

```{r cell-import}
data(cells, package = "modeldata")
cells
```

Random forest models typically perform well with defaults, but the accuracy of some other kinds of models, such as boosted tree models or decision tree models, can be sensitive to the values of hyperparameters. In this article we will train a **decision tree** model. There are several hyperparameters for decision tree models that can be tuned for better performance. Let's explore:

- the complexity parameter (which we call `cost_complexity` in tidymodels) for the tree, and
- the maximum `tree_depth`.

Before we start the tuning process, we split our data into training and testing sets, just like when we trained the model with one default set of hyperparameters. We can use `strata = class` if we want our training and testing sets to be created using stratified sampling so that both have the same proportion of both kinds of segmentation.

```{r cell-split}
library(tidymodels) 

set.seed(123)
cell_split <- initial_split(cells %>% select(-case), strata = class)
cell_train <- training(cell_split)
cell_test  <- testing(cell_split)
```

We use the training data for tuning the model.

## Tuning hyperparameters

To tune the decision tree hyperparameters `cost_complexity` and `tree_depth`, we create a model specification that identifies which model parameters we will `tune()`.

```{r tune-spec}
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tune_spec
```

We can't train this specification on a single data set (such as the entire training set) and learn what the hyperparameter values should be, but we can train many models using resampled data and see which models turn out best. We can create a regular grid of values to try using some convenience functions for each hyperparameter:

```{r tree-grid}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
```

Let's create cross-validation folds for tuning, and then use `tune_grid()` to fit models at all the different values we chose for each tuned hyperparameter.

```{r tree-res, dependson=c("tune-spec", "cell-split")}
set.seed(234)
cell_folds <- vfold_cv(cell_train)

set.seed(345)
tree_res <- tune_spec %>%
  tune_grid(
    class ~ .,
    resamples = cell_folds,
    grid = tree_grid
  )

tree_res
```

Once we have our tuning results, we can both explore them through visualization and then select the best result.

```{r best-tree, dependson="tree-res", fig.width=8, fig.height=7}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.5) +
  geom_point() +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10()

best_tree <- tree_res %>%
  select_best("roc_auc")

best_tree
```

These are the values for `tree_depth` and `cost_complexity` that maximize AUC in this data set of cell images. We can update (or "finalize") our model with these values.

```{r final-mod, dependson="best-tree"}
final_mod <- finalize_model(
  tune_spec,
  best_tree
)

final_mod
```

Our tuning is done!

## Exploring results

Let's fit this final model to the training data. What does the decision tree look like?

```{r final-tree, dependson="final-mod"}
final_tree <- final_mod %>%
  fit(class ~ .,
      data = cell_train) 

final_tree
```

We can also visualize the decision tree using the [partykit](http://partykit.r-forge.r-project.org/partykit/) package.

```{r eval=FALSE}
library(partykit)

tree_party <- as.party(final_tree$fit) ## currently broken
plot(tree_party)
```


Perhaps we would also like to understand what variables are important in this final model. We can use the [vip](https://koalaverse.github.io/vip/) package to estimate variable importance.

```{r vip, dependson="final-tree", fig.width=6, fig.height=5}
library(vip)

final_tree %>%
  vip(geom = "point")
```

These are the automated image analysis measurements that are the most important in driving segmentation quality predictions.

Finally, let's return to our test data and estimate the model performance we expect to see on new data. We can use the function `last_fit()` with our finalized model; this function _fits_ the finalized model on the training data and _evaluates_ the finalized model on the testing data.

```{r last-fit, dependson=c("final_mod", "cell-split")}
final_mod %>%
  last_fit(class ~ ., 
           cell_split) %>%
  collect_metrics
```

The performance metrics from the test set indicate that we did not overfit during our tuning procedure.

