---
title: "Using parsnip for Classification Models"
tags: [rsample, parsnip]
categories: [model fitting]
type: learn-subsection
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
source(here::here("content/learn/common.R"))
```
  
```{r load, include=FALSE}
library(tidymodels)
pkgs <- c("tidymodels", "keras")

theme_set(theme_bw() + theme(legend.position = "top"))
```

`r req_pkgs(pkgs)` You will also need the python keras library installed (see `?keras::install_keras()`).

# Introduction

This article focuses on fitting models using the parsnip package. For a classification problem, a single model is fit and evaluated using a validation set. While the tune package has functionality to also do this, the parsnip package is the center of attention so that we can better understand its usage. 

# Fitting a neural network model


A small, two predictor classification data set is used to fit the model. The data are in the workflows package and have been split into a training, validation, and test data sets. In this analysis, the test set is left untouched; the code here is trying to emulate a good data usage methodology where the test set would only be evaluated once a variety of models were considered. 


```{r biv--split}
data(bivariate)
nrow(bivariate_train)
nrow(bivariate_val)
```

A plot of the data shows two right-skewed predictors: 

```{r biv-plot, fig.width = 6, fig.height = 6.1}
ggplot(bivariate_train, aes(x = A, y = B, col = Class)) + 
  geom_point(alpha = .2)
```

A single hidden layer neural network will be used to predict the outcome. To do so, the columns of the predictor are transformed to be more symmetric (via the `step_BoxCox()` function) and on a common scale (using `step_normalize()`). recipes will be used to do so:

```{r biv--proc}
biv_rec <- 
  recipe(Class ~ ., data = bivariate_train) %>%
  # There are some missing values to be imputed: 
  step_BoxCox(all_predictors())%>%
  step_normalize(all_predictors()) %>%
  # Estimate the means and standard deviations for the columns as well as
  # the two transformation parameters: 
  prep(training = bivariate_train, retain = TRUE)

# juice() will be used to get the processed training set back

val_normalized <- bake(biv_rec, new_data = bivariate_val, all_predictors())
# For when we arrive at a final model: 
test_normalized <- bake(biv_rec, new_data = bivariate_test, all_predictors())
```

The keras package will be used to fit a model with 5 hidden units and uses a 10% dropout rate to regularize the model:

```{r biv--nnet}
set.seed(57974)
nnet_fit <-
  mlp(epochs = 100, hidden_units = 5, dropout = 0.1) %>%
  set_mode("classification") %>% 
  # Also set engine-specific argument to prevent logging the results: 
  set_engine("keras", verbose = 0) %>%
  fit(Class ~ ., data = juice(biv_rec))

nnet_fit
```

In parsnip, the `predict()` function can be used to characterize performance on the validation set. Since parsnip always produces tibble outputs, these can just be column bound to the original data: 

```{r biv--perf}
val_results <- 
  bivariate_val %>%
  bind_cols(
    predict(nnet_fit, new_data = val_normalized),
    predict(nnet_fit, new_data = val_normalized, type = "prob")
  )
val_results %>% slice(1:5)

val_results %>% roc_auc(truth = Class, .pred_One)
val_results %>% accuracy(truth = Class, .pred_class)
val_results %>% conf_mat(truth = Class, .pred_class)
```

Let's also create a grid to get a visual sense of the class boundary for the validation set.

```{r biv-boundary, fig.width = 6, fig.height = 6.1}
a_rng <- range(bivariate_train$A)
b_rng <- range(bivariate_train$B)
x_grid <-
  expand.grid(A = seq(a_rng[1], a_rng[2], length.out = 100),
              B = seq(b_rng[1], b_rng[2], length.out = 100))
x_grid_trans <- bake(biv_rec, x_grid)

# Make predictions using the transformed predictors but 
# attach them to the predictors in the original units: 
x_grid <- 
  x_grid %>% 
  bind_cols(predict(nnet_fit, x_grid_trans, type = "prob"))

ggplot(x_grid, aes(x = A, y = B)) + 
  geom_contour(aes(z = .pred_One), breaks = .5, col = "black") + 
  geom_point(data = bivariate_val, aes(col = Class), alpha = 0.3)
```



# Session information

```{r si, echo = FALSE}
small_session(pkgs)
```
