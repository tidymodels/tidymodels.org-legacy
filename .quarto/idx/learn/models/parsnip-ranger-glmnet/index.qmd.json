{"title":"Regression models two ways","markdown":{"yaml":{"title":"Regression models two ways","categories":["model fitting","random forests","linear regression"],"type":"learn-subsection","weight":1,"description":"Create and train different kinds of regression models with different computational engines.\n","toc":true,"toc-depth":2,"include-after-body":"../../../resources.html"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n\n```{r}\n#| label: \"load\"\n#| include: false\nlibrary(tidymodels)\nlibrary(ranger)\nlibrary(randomForest)\nlibrary(glmnet)\n\npreds <- c(\"Longitude\", \"Latitude\", \"Lot_Area\", \"Neighborhood\", \"Year_Sold\")\npred_names <- paste0(\"`\", preds, \"`\")\n\npkgs <- c(\"tidymodels\", \"ranger\", \"randomForest\", \"glmnet\")\n\ntheme_set(theme_bw() + theme(legend.position = \"top\"))\n```\n\n\n\n`r article_req_pkgs(pkgs)`\n\nWe can create regression models with the tidymodels package [parsnip](https://parsnip.tidymodels.org/) to predict continuous or numeric quantities. Here, let's first fit a random forest model, which does _not_ require all numeric input (see discussion [here](https://bookdown.org/max/FES/categorical-trees.html)) and discuss how to use `fit()` and `fit_xy()`, as well as _data descriptors_. \n\nSecond, let's fit a regularized linear regression model to demonstrate how to move between different types of models using parsnip. \n\n## The Ames housing data\n\nWe'll use the Ames housing data set to demonstrate how to create regression models using parsnip. First, set up the data set and create a simple training/test set split:\n\n```{r}\n#| label: \"ames-split\"\nlibrary(tidymodels)\n\ndata(ames)\n\nset.seed(4595)\ndata_split <- initial_split(ames, strata = \"Sale_Price\", prop = 0.75)\n\names_train <- training(data_split)\names_test  <- testing(data_split)\n```\n\nThe use of the test set here is _only for illustration_; normally in a data analysis these data would be saved to the very end after many models have been evaluated. \n\n## Random forest\n\nWe'll start by fitting a random forest model to a small set of parameters. Let's create a model with the predictors `r knitr::combine_words(pred_names)`. A simple random forest model can be specified via:\n\n```{r}\n#| label: \"rf-basic\"\nrf_defaults <- rand_forest(mode = \"regression\")\nrf_defaults\n```\n\nThe model will be fit with the ranger package by default. Since we didn't add any extra arguments to `fit`, _many_ of the arguments will be set to their defaults from the function  `ranger::ranger()`. The help pages for the model function describe the default parameters and you can also use the `translate()` function to check out such details. \n\nThe parsnip package provides two different interfaces to fit a model: \n\n- the formula interface (`fit()`), and\n- the non-formula interface (`fit_xy()`).\n\nLet's start with the non-formula interface:\n\n\n```{r}\n#| label: \"rf-basic-xy\"\npreds <- c(\"Longitude\", \"Latitude\", \"Lot_Area\", \"Neighborhood\", \"Year_Sold\")\n\nrf_xy_fit <- \n  rf_defaults %>%\n  set_engine(\"ranger\") %>%\n  fit_xy(\n    x = ames_train[, preds],\n    y = log10(ames_train$Sale_Price)\n  )\n\nrf_xy_fit\n```\n\nThe non-formula interface doesn't do anything to the predictors before passing them to the underlying model function. This particular model does _not_ require indicator variables (sometimes called \"dummy variables\") to be created prior to fitting the model. Note that the output shows \"Number of independent variables:  5\".\n\nFor regression models, we can use the basic `predict()` method, which returns a tibble with a column named `.pred`:\n\n```{r}\n#| label: \"rf-basic-xy-pred\"\ntest_results <- \n  ames_test %>%\n  select(Sale_Price) %>%\n  mutate(Sale_Price = log10(Sale_Price)) %>%\n  bind_cols(\n    predict(rf_xy_fit, new_data = ames_test[, preds])\n  )\ntest_results %>% slice(1:5)\n\n# summarize performance\ntest_results %>% metrics(truth = Sale_Price, estimate = .pred) \n```\n\nNote that: \n\n * If the model required indicator variables, we would have to create them manually prior to using `fit()` (perhaps using the recipes package).\n * We had to manually log the outcome prior to modeling. \n\nNow, for illustration, let's use the formula method using some new parameter values:\n\n```{r}\n#| label: \"rf-basic-form\"\nrand_forest(mode = \"regression\", mtry = 3, trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  fit(\n    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,\n    data = ames_train\n  )\n```\n \nSuppose that we would like to use the randomForest package instead of ranger. To do so, the only part of the syntax that needs to change is the `set_engine()` argument:\n\n\n```{r}\n#| label: \"rf-rf\"\nrand_forest(mode = \"regression\", mtry = 3, trees = 1000) %>%\n  set_engine(\"randomForest\") %>%\n  fit(\n    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,\n    data = ames_train\n  )\n```\n\nLook at the formula code that was printed out; one function uses the argument name `ntree` and the other uses `num.trees`. The parsnip models don't require you to know the specific names of the main arguments. \n\nNow suppose that we want to modify the value of `mtry` based on the number of predictors in the data. Usually, a good default value is `floor(sqrt(num_predictors))` but a pure bagging model requires an `mtry` value equal to the total number of parameters. There may be cases where you may not know how many predictors are going to be present when the model will be fit (perhaps due to the generation of indicator variables or a variable filter) so this might be difficult to know exactly ahead of time when you write your code. \n\nWhen the model it being fit by parsnip, [_data descriptors_](https://parsnip.tidymodels.org/reference/descriptors.html) are made available. These attempt to let you know what you will have available when the model is fit. When a model object is created (say using `rand_forest()`), the values of the arguments that you give it are _immediately evaluated_ unless you delay them. To delay the evaluation of any argument, you can used `rlang::expr()` to make an expression. \n\nTwo relevant data descriptors for our example model are:\n\n * `.preds()`: the number of predictor _variables_ in the data set that are associated with the predictors **prior to dummy variable creation**.\n * `.cols()`: the number of predictor _columns_ after dummy variables (or other encodings) are created.\n\nSince ranger won't create indicator values, `.preds()` would be appropriate for `mtry` for a bagging model. \n\nFor example, let's use an expression with the `.preds()` descriptor to fit a bagging model: \n\n```{r}\n#| label: \"bagged\"\nrand_forest(mode = \"regression\", mtry = .preds(), trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  fit(\n    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,\n    data = ames_train\n  )\n```\n\n\n## Regularized regression\n\nA linear model might work for this data set as well. We can use the `linear_reg()` parsnip model. There are two engines that can perform regularization/penalization, the glmnet and sparklyr packages. Let's use the former here. The glmnet package only implements a non-formula method, but parsnip will allow either one to be used. \n\nWhen regularization is used, the predictors should first be centered and scaled before being passed to the model. The formula method won't do that automatically so we will need to do this ourselves. We'll use the [recipes](https://recipes.tidymodels.org/) package for these steps. \n\n```{r}\n#| label: \"glmn-form\"\nnorm_recipe <- \n  recipe(\n    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, \n    data = ames_train\n  ) %>%\n  step_other(Neighborhood) %>% \n  step_dummy(all_nominal()) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors()) %>%\n  step_log(Sale_Price, base = 10) %>% \n  # estimate the means and standard deviations\n  prep(training = ames_train, retain = TRUE)\n\n# Now let's fit the model using the processed version of the data\n\nglmn_fit <- \n  linear_reg(penalty = 0.001, mixture = 0.5) %>% \n  set_engine(\"glmnet\") %>%\n  fit(Sale_Price ~ ., data = bake(norm_recipe, new_data = NULL))\nglmn_fit\n```\n\nIf `penalty` were not specified, all of the `lambda` values would be computed. \n\nTo get the predictions for this specific value of `lambda` (aka `penalty`):\n\n```{r}\n#| label: \"glmn-pred\"\n# First, get the processed version of the test set predictors:\ntest_normalized <- bake(norm_recipe, new_data = ames_test, all_predictors())\n\ntest_results <- \n  test_results %>%\n  rename(`random forest` = .pred) %>%\n  bind_cols(\n    predict(glmn_fit, new_data = test_normalized) %>%\n      rename(glmnet = .pred)\n  )\ntest_results\n\ntest_results %>% metrics(truth = Sale_Price, estimate = glmnet) \n\ntest_results %>% \n  gather(model, prediction, -Sale_Price) %>% \n  ggplot(aes(x = prediction, y = Sale_Price)) + \n  geom_abline(col = \"green\", lty = 2) + \n  geom_point(alpha = .4) + \n  facet_wrap(~model) + \n  coord_fixed()\n```\n\nThis final plot compares the performance of the random forest and regularized regression models.\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n \n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n\n```{r}\n#| label: \"load\"\n#| include: false\nlibrary(tidymodels)\nlibrary(ranger)\nlibrary(randomForest)\nlibrary(glmnet)\n\npreds <- c(\"Longitude\", \"Latitude\", \"Lot_Area\", \"Neighborhood\", \"Year_Sold\")\npred_names <- paste0(\"`\", preds, \"`\")\n\npkgs <- c(\"tidymodels\", \"ranger\", \"randomForest\", \"glmnet\")\n\ntheme_set(theme_bw() + theme(legend.position = \"top\"))\n```\n\n\n## Introduction\n\n`r article_req_pkgs(pkgs)`\n\nWe can create regression models with the tidymodels package [parsnip](https://parsnip.tidymodels.org/) to predict continuous or numeric quantities. Here, let's first fit a random forest model, which does _not_ require all numeric input (see discussion [here](https://bookdown.org/max/FES/categorical-trees.html)) and discuss how to use `fit()` and `fit_xy()`, as well as _data descriptors_. \n\nSecond, let's fit a regularized linear regression model to demonstrate how to move between different types of models using parsnip. \n\n## The Ames housing data\n\nWe'll use the Ames housing data set to demonstrate how to create regression models using parsnip. First, set up the data set and create a simple training/test set split:\n\n```{r}\n#| label: \"ames-split\"\nlibrary(tidymodels)\n\ndata(ames)\n\nset.seed(4595)\ndata_split <- initial_split(ames, strata = \"Sale_Price\", prop = 0.75)\n\names_train <- training(data_split)\names_test  <- testing(data_split)\n```\n\nThe use of the test set here is _only for illustration_; normally in a data analysis these data would be saved to the very end after many models have been evaluated. \n\n## Random forest\n\nWe'll start by fitting a random forest model to a small set of parameters. Let's create a model with the predictors `r knitr::combine_words(pred_names)`. A simple random forest model can be specified via:\n\n```{r}\n#| label: \"rf-basic\"\nrf_defaults <- rand_forest(mode = \"regression\")\nrf_defaults\n```\n\nThe model will be fit with the ranger package by default. Since we didn't add any extra arguments to `fit`, _many_ of the arguments will be set to their defaults from the function  `ranger::ranger()`. The help pages for the model function describe the default parameters and you can also use the `translate()` function to check out such details. \n\nThe parsnip package provides two different interfaces to fit a model: \n\n- the formula interface (`fit()`), and\n- the non-formula interface (`fit_xy()`).\n\nLet's start with the non-formula interface:\n\n\n```{r}\n#| label: \"rf-basic-xy\"\npreds <- c(\"Longitude\", \"Latitude\", \"Lot_Area\", \"Neighborhood\", \"Year_Sold\")\n\nrf_xy_fit <- \n  rf_defaults %>%\n  set_engine(\"ranger\") %>%\n  fit_xy(\n    x = ames_train[, preds],\n    y = log10(ames_train$Sale_Price)\n  )\n\nrf_xy_fit\n```\n\nThe non-formula interface doesn't do anything to the predictors before passing them to the underlying model function. This particular model does _not_ require indicator variables (sometimes called \"dummy variables\") to be created prior to fitting the model. Note that the output shows \"Number of independent variables:  5\".\n\nFor regression models, we can use the basic `predict()` method, which returns a tibble with a column named `.pred`:\n\n```{r}\n#| label: \"rf-basic-xy-pred\"\ntest_results <- \n  ames_test %>%\n  select(Sale_Price) %>%\n  mutate(Sale_Price = log10(Sale_Price)) %>%\n  bind_cols(\n    predict(rf_xy_fit, new_data = ames_test[, preds])\n  )\ntest_results %>% slice(1:5)\n\n# summarize performance\ntest_results %>% metrics(truth = Sale_Price, estimate = .pred) \n```\n\nNote that: \n\n * If the model required indicator variables, we would have to create them manually prior to using `fit()` (perhaps using the recipes package).\n * We had to manually log the outcome prior to modeling. \n\nNow, for illustration, let's use the formula method using some new parameter values:\n\n```{r}\n#| label: \"rf-basic-form\"\nrand_forest(mode = \"regression\", mtry = 3, trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  fit(\n    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,\n    data = ames_train\n  )\n```\n \nSuppose that we would like to use the randomForest package instead of ranger. To do so, the only part of the syntax that needs to change is the `set_engine()` argument:\n\n\n```{r}\n#| label: \"rf-rf\"\nrand_forest(mode = \"regression\", mtry = 3, trees = 1000) %>%\n  set_engine(\"randomForest\") %>%\n  fit(\n    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,\n    data = ames_train\n  )\n```\n\nLook at the formula code that was printed out; one function uses the argument name `ntree` and the other uses `num.trees`. The parsnip models don't require you to know the specific names of the main arguments. \n\nNow suppose that we want to modify the value of `mtry` based on the number of predictors in the data. Usually, a good default value is `floor(sqrt(num_predictors))` but a pure bagging model requires an `mtry` value equal to the total number of parameters. There may be cases where you may not know how many predictors are going to be present when the model will be fit (perhaps due to the generation of indicator variables or a variable filter) so this might be difficult to know exactly ahead of time when you write your code. \n\nWhen the model it being fit by parsnip, [_data descriptors_](https://parsnip.tidymodels.org/reference/descriptors.html) are made available. These attempt to let you know what you will have available when the model is fit. When a model object is created (say using `rand_forest()`), the values of the arguments that you give it are _immediately evaluated_ unless you delay them. To delay the evaluation of any argument, you can used `rlang::expr()` to make an expression. \n\nTwo relevant data descriptors for our example model are:\n\n * `.preds()`: the number of predictor _variables_ in the data set that are associated with the predictors **prior to dummy variable creation**.\n * `.cols()`: the number of predictor _columns_ after dummy variables (or other encodings) are created.\n\nSince ranger won't create indicator values, `.preds()` would be appropriate for `mtry` for a bagging model. \n\nFor example, let's use an expression with the `.preds()` descriptor to fit a bagging model: \n\n```{r}\n#| label: \"bagged\"\nrand_forest(mode = \"regression\", mtry = .preds(), trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  fit(\n    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,\n    data = ames_train\n  )\n```\n\n\n## Regularized regression\n\nA linear model might work for this data set as well. We can use the `linear_reg()` parsnip model. There are two engines that can perform regularization/penalization, the glmnet and sparklyr packages. Let's use the former here. The glmnet package only implements a non-formula method, but parsnip will allow either one to be used. \n\nWhen regularization is used, the predictors should first be centered and scaled before being passed to the model. The formula method won't do that automatically so we will need to do this ourselves. We'll use the [recipes](https://recipes.tidymodels.org/) package for these steps. \n\n```{r}\n#| label: \"glmn-form\"\nnorm_recipe <- \n  recipe(\n    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, \n    data = ames_train\n  ) %>%\n  step_other(Neighborhood) %>% \n  step_dummy(all_nominal()) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors()) %>%\n  step_log(Sale_Price, base = 10) %>% \n  # estimate the means and standard deviations\n  prep(training = ames_train, retain = TRUE)\n\n# Now let's fit the model using the processed version of the data\n\nglmn_fit <- \n  linear_reg(penalty = 0.001, mixture = 0.5) %>% \n  set_engine(\"glmnet\") %>%\n  fit(Sale_Price ~ ., data = bake(norm_recipe, new_data = NULL))\nglmn_fit\n```\n\nIf `penalty` were not specified, all of the `lambda` values would be computed. \n\nTo get the predictions for this specific value of `lambda` (aka `penalty`):\n\n```{r}\n#| label: \"glmn-pred\"\n# First, get the processed version of the test set predictors:\ntest_normalized <- bake(norm_recipe, new_data = ames_test, all_predictors())\n\ntest_results <- \n  test_results %>%\n  rename(`random forest` = .pred) %>%\n  bind_cols(\n    predict(glmn_fit, new_data = test_normalized) %>%\n      rename(glmnet = .pred)\n  )\ntest_results\n\ntest_results %>% metrics(truth = Sale_Price, estimate = glmnet) \n\ntest_results %>% \n  gather(model, prediction, -Sale_Price) %>% \n  ggplot(aes(x = prediction, y = Sale_Price)) + \n  geom_abline(col = \"green\", lty = 2) + \n  geom_point(alpha = .4) + \n  facet_wrap(~model) + \n  coord_fixed()\n```\n\nThis final plot compares the performance of the random forest and regularized regression models.\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"include-after-body":["../../../resources.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","notebook-preview-download":"Download Notebook","notebook-preview-back":"Back to Article"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.92","theme":["cosmo","../../../styles.scss","../../../styles-frontpage.scss"],"quarto-required":">= 1.3.353","linestretch":1.6,"grid":{"body-width":"840px"},"title":"Regression models two ways","categories":["model fitting","random forests","linear regression"],"type":"learn-subsection","weight":1,"description":"Create and train different kinds of regression models with different computational engines.\n"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}