{"title":"Working with model coefficients","markdown":{"yaml":{"title":"Working with model coefficients","categories":["model fitting","tidying results","linear regression","model tuning"],"type":"learn-subsection","weight":5,"description":"Create models that use coefficients, extract them from fitted models, and visualize them.\n","toc":true,"toc-depth":2,"include-after-body":"../../../resources.html"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\npkgs <- c(\"tidymodels\", \"glmnet\")\nlibrary(Matrix)\nlibrary(glmnet)\n```\n\n\nThere are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients. \n\nThis article describes how to retrieve the estimated coefficients from models fit using tidymodels. `r article_req_pkgs(pkgs)`\n\n## Linear regression\n\nLet's start with a linear regression model: \n\n$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p$$ \n\nThe $\\beta$ values are the coefficients and the $x_j$ are model predictors, or features. \n\nLet's use the [Chicago train data](https://bookdown.org/max/FES/chicago-intro.html) where we predict the ridership at the Clark and Lake station (column name: `ridership`) with the previous ridership data 14 days prior at three of the stations. \n\nThe data are in the modeldata package:  \n\n```{r}\n#| label: \"setup-tm\"\n#| message: false\n#| warning: false\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n\ndata(Chicago)\n\nChicago <- Chicago %>% select(ridership, Clark_Lake, Austin, Harlem)\n```\n\n### A single model\n\nLet's start by fitting only a single parsnip model object. We'll create a model specification using `linear_reg()`. \n\n::: {.callout-note}\nThe default engine is `\"lm\"` so no call to `set_engine()` is required. \n:::\n\nThe `fit()` function estimates the model coefficients, given a formula and data set. \n\n\n```{r}\n#| label: \"lm-single\"\nlm_spec <- linear_reg()\nlm_fit <- fit(lm_spec, ridership ~ ., data = Chicago)\nlm_fit\n```\n\nThe best way to retrieve the fitted parameters is to use the `tidy()` method. This function, in the broom package, returns the coefficients and their associated statistics in a data frame with standardized column names: \n\n```{r}\n#| label: \"lm-tidy\"\ntidy(lm_fit)\n```\n\nWe'll use this function in subsequent sections. \n\n### Resampled or tuned models\n\nThe tidymodels framework emphasizes the use of resampling methods to evaluate and characterize how well a model works. While time series resampling methods are appropriate for these data, we can also use the [bootstrap](https://www.tmwr.org/resampling.html#bootstrap) to resample the data. This is a standard resampling approach when evaluating the uncertainty in statistical estimates.  \n\nWe'll use five bootstrap resamples of the data to simplify the plots and output (normally, we would use a larger number of resamples for more reliable estimates).\n\n```{r}\n#| label: \"bootstraps\"\nset.seed(123)\nbt <- bootstraps(Chicago, times = 5)\n```\n\nWith resampling, we fit the same model to the different simulated versions of the data set produced by resampling. The tidymodels function [`fit_resamples()`](https://www.tmwr.org/resampling.html#resampling-performance) is the recommended approach for doing so. \n\n::: {.callout-warning}\n The `fit_resamples()` function does not automatically save the model objects for each resample since these can be quite large and its main purpose is estimating performance. However, we can pass a function to `fit_resamples()` that _can_ save the model object or any other aspect of the fit. \n:::\n\nThis function takes a single argument that represents the fitted [workflow object](https://www.tmwr.org/workflows.html) (even if you don't give `fit_resamples()` a workflow).\n\nFrom this, we can extract the model fit. There are two \"levels\" of model objects that are available: \n\n* The parsnip model object, which wraps the underlying model object. We retrieve this using the `extract_fit_parsnip()` function. \n\n* The underlying model object (a.k.a. the engine fit) via the `extract_fit_engine()`. \n\nWe'll use the latter option and then tidy this model object as we did in the previous section. Let's add this to the control function so that we can re-use it. \n\n```{r}\n#| label: \"lm-ctrl\"\nget_lm_coefs <- function(x) {\n  x %>% \n    # get the lm model object\n    extract_fit_engine() %>% \n    # transform its format\n    tidy()\n}\ntidy_ctrl <- control_grid(extract = get_lm_coefs)\n```\n\nThis argument is then passed to `fit_resamples()`:\n\n```{r}\n#| label: \"lm-resampled\"\nlm_res <- \n  lm_spec %>% \n  fit_resamples(ridership ~ ., resamples = bt, control = tidy_ctrl)\nlm_res\n```\n\nNote that there is a `.extracts` column in our resampling results. This object contains the output of our `get_lm_coefs()` function for each resample. The structure of the elements of this column is a little complex. Let's start by looking at the first element (which corresponds to the first resample): \n\n\n```{r}\n#| label: \"lm-extract-ex\"\nlm_res$.extracts[[1]]\n```\n\nThere is _another_ column in this element called `.extracts` that has the results of the `tidy()` function call: \n\n```{r}\n#| label: \"lm-extract-again\"\nlm_res$.extracts[[1]]$.extracts[[1]]\n```\n\nThese nested columns can be flattened via the purrr `unnest()` function: \n\n```{r}\n#| label: \"lm-extract-almost\"\nlm_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) \n```\n\nWe still have a column of nested tibbles, so we can run the same command again to get the data into a more useful format: \n\n```{r}\n#| label: \"lm-extract-final\"\nlm_coefs <- \n  lm_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) %>% \n  unnest(.extracts)\n\nlm_coefs %>% select(id, term, estimate, p.value)\n```\n\nThat's better! Now, let's plot the model coefficients for each resample: \n\n```{r}\n#| label: \"lm-plot\"\nlm_coefs %>%\n  filter(term != \"(Intercept)\") %>% \n  ggplot(aes(x = term, y = estimate, group = id, col = id)) +  \n  geom_hline(yintercept = 0, lty = 3) + \n  geom_line(alpha = 0.3, lwd = 1.2) + \n  labs(y = \"Coefficient\", x = NULL) +\n  theme(legend.position = \"top\")\n```\n\nThere seems to be a lot of uncertainty in the coefficient for the Austin station data, but less for the other two. \n\nLooking at the code for unnesting the results, you may find the double-nesting structure excessive or cumbersome. However, the extraction functionality is flexible, and a simpler structure would prevent many use cases. \n\n## More complex: a glmnet model\n\nThe glmnet model can fit the same linear regression model structure shown above. It uses regularization (a.k.a penalization) to estimate the model parameters. This has the benefit of shrinking the coefficients towards zero, important in situations where there are strong correlations between predictors or if some feature selection is required. Both of these cases are true for our Chicago train data set. \n\nThere are two types of penalization that this model uses: \n\n* Lasso (a.k.a. $L_1$) penalties can shrink the model terms so much that they are absolute zero (i.e. their effect is entirely removed from the model). \n\n* Weight decay (a.k.a ridge regression or $L_2$) uses a different type of penalty that is most useful for highly correlated predictors. \n\nThe glmnet model has two primary tuning parameters, the total amount of penalization and the mixture of the two penalty types. For example, this specification:\n\n```{r}\n#| label: \"glmnet-spec\"\nglmnet_spec <- \n  linear_reg(penalty = 0.1, mixture = 0.95) %>% \n  set_engine(\"glmnet\")\n```\n\nhas a penalty that is 95% lasso and 5% weight decay. The total amount of these two penalties is 0.1 (which is fairly high). \n\n::: {.callout-note}\nModels with regularization require that predictors are all on the same scale. The ridership at our three stations are very different, but glmnet [automatically centers and scales the data](https://parsnip.tidymodels.org/reference/details_linear_reg_glmnet.html). You can use recipes to [center and scale your data yourself](https://recipes.tidymodels.org/reference/step_normalize.html). \n:::\n\nLet's combine the model specification with a formula in a model `workflow()` and then fit the model to the data:\n\n```{r}\n#| label: \"glmnet-wflow\"\nglmnet_wflow <- \n  workflow() %>% \n  add_model(glmnet_spec) %>% \n  add_formula(ridership ~ .)\n\nglmnet_fit <- fit(glmnet_wflow, Chicago)\nglmnet_fit\n```\n\nIn this output, the term `lambda` is used to represent the penalty. \n\nNote that the output shows many values of the penalty despite our specification of `penalty = 0.1`. It turns out that this model fits a \"path\" of penalty values.  Even though we are interested in a value of 0.1, we can get the model coefficients for many associated values of the penalty from the same model object. \n\nLet's look at two different approaches to obtaining the coefficients. Both will use the `tidy()` method. One will tidy a glmnet object and the other will tidy a tidymodels object. \n\n### Using glmnet penalty values\n\nThis glmnet fit contains multiple penalty values which depend on the data set; changing the data (or the mixture amount) often produces a different set of values. For this data set, there are `r length(extract_fit_engine(glmnet_fit)$lambda)` penalties available. To get the set of penalties produced for this data set, we can extract the engine fit and tidy: \n\n```{r}\n#| label: \"glmnet-tidy\"\nglmnet_fit %>% \n  extract_fit_engine() %>% \n  tidy() %>% \n  rename(penalty = lambda) %>%   # <- for consistent naming\n  filter(term != \"(Intercept)\")\n```\n\nThis works well but, it turns out that our penalty value (0.1) is not in the list produced by the model! The underlying package has functions that use interpolation to produce coefficients for this specific value, but the `tidy()` method for glmnet objects does not use it. \n\n### Using specific penalty values\n\nIf we run the `tidy()` method on the workflow or parsnip object, a different function is used that returns the coefficients for the penalty value that we specified: \n\n```{r}\n#| label: \"glmnet-tidy-parsnip\"\ntidy(glmnet_fit)\n```\n\nFor any another (single) penalty, we can use an additional argument:\n\n```{r}\n#| label: \"glmnet-tidy-parsnip-alt\"\ntidy(glmnet_fit, penalty = 5.5620)  # A value from above\n```\n\nThe reason for having two `tidy()` methods is that, with tidymodels, the focus is on using a specific penalty value. \n\n\n### Tuning a glmnet model\n\nIf we know a priori acceptable values for penalty and mixture, we can use the `fit_resamples()` function as we did before with linear regression. Otherwise, we can tune those parameters with the tidymodels `tune_*()` functions. \n\nLet's tune our glmnet model over both parameters with this grid: \n\n```{r}\n#| label: \"glmnet-grid\"\npen_vals <- 10^seq(-3, 0, length.out = 10)\ngrid <- crossing(penalty = pen_vals, mixture = c(0.1, 1.0))\n```\n\nHere is where more glmnet-related complexity comes in: we know that each resample and each value of `mixture` will probably produce a different set of penalty values contained in the model object. _How can we look at the coefficients at the specific penalty values that we are using to tune?_\n\nThe approach that we suggest is to use the special `path_values` option for glmnet. Details are described in the [technical documentation about glmnet and tidymodels](https://parsnip.tidymodels.org/reference/glmnet-details.html#arguments) but in short, this parameter will assign the collection of penalty values used by each glmnet fit (regardless of the data or value of mixture). \n\nWe can pass these as an engine argument and then update our previous workflow object:\n\n```{r}\n#| label: \"glmnet-tune\"\nglmnet_tune_spec <- \n  linear_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\", path_values = pen_vals)\n\nglmnet_wflow <- \n  glmnet_wflow %>% \n  update_model(glmnet_tune_spec)\n```\n\nNow we will use an extraction function similar to when we used ordinary least squares. We add an additional argument to retain coefficients that are shrunk to zero by the lasso penalty: \n\n```{r}\n#| label: \"glmnet-tuning\"\nget_glmnet_coefs <- function(x) {\n  x %>% \n    extract_fit_engine() %>% \n    tidy(return_zeros = TRUE) %>% \n    rename(penalty = lambda)\n}\nparsnip_ctrl <- control_grid(extract = get_glmnet_coefs)\n\nglmnet_res <- \n  glmnet_wflow %>% \n  tune_grid(\n    resamples = bt,\n    grid = grid,\n    control = parsnip_ctrl\n  )\nglmnet_res\n```\n\nAs noted before, the elements of the main `.extracts` column have an embedded list column with the results of `get_glmnet_coefs()`:  \n\n```{r}\n#| label: \"glmnet-extract-single\"\nglmnet_res$.extracts[[1]] %>% head()\n\nglmnet_res$.extracts[[1]]$.extracts[[1]] %>% head()\n```\n\nAs before, we'll have to use a double `unnest()`. Since the penalty value is in both the top-level and lower-level `.extracts`, we'll use `select()` to get rid of the first version (but keep `mixture`):\n\n```{r}\n#| label: \"glmnet-extract-1\"\n#| eval: false\nglmnet_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) %>% \n  select(id, mixture, .extracts) %>%  # <- removes the first penalty column\n  unnest(.extracts)\n```\n\nBut wait! We know that each glmnet fit contains all of the coefficients. This means, for a specific resample and value of `mixture`, the results are the same:  \n\n```{r}\n#| label: \"glmnet-extract-dups\"\nall.equal(\n  # First bootstrap, first `mixture`, first `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[1]],\n  # First bootstrap, first `mixture`, second `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[2]]\n)\n```\n\nFor this reason, we'll add a `slice(1)` when grouping by `id` and `mixture`. This will get rid of the replicated results. \n\n```{r}\n#| label: \"glmnet-extract-final\"\nglmnet_coefs <- \n  glmnet_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) %>% \n  select(id, mixture, .extracts) %>% \n  group_by(id, mixture) %>%          # ┐\n  slice(1) %>%                       # │ Remove the redundant results\n  ungroup() %>%                      # ┘\n  unnest(.extracts)\n\nglmnet_coefs %>% \n  select(id, penalty, mixture, term, estimate) %>% \n  filter(term != \"(Intercept)\")\n```\n\nNow we have the coefficients. Let's look at how they behave as more regularization is used: \n\n```{r}\n#| label: \"glmnet-plot\"\n#| fig-height:  4\n#| fig-width:  8.5\nglmnet_coefs %>% \n  filter(term != \"(Intercept)\") %>% \n  mutate(mixture = format(mixture)) %>% \n  ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) + \n  geom_hline(yintercept = 0, lty = 3) +\n  geom_line(alpha = 0.5, lwd = 1.2) + \n  facet_wrap(~ term) + \n  scale_x_log10() +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(y = \"coefficient\") +\n  theme(legend.position = \"top\")\n```\n\nNotice a couple of things: \n\n* With a pure lasso model (i.e., `mixture = 1`), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases. Also, as the penalty increases, the uncertainty in this coefficient decreases. \n\n* The Harlem predictor is either quickly selected out of the model or goes from negative to positive. \n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\npkgs <- c(\"tidymodels\", \"glmnet\")\nlibrary(Matrix)\nlibrary(glmnet)\n```\n\n## Introduction \n\nThere are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients. \n\nThis article describes how to retrieve the estimated coefficients from models fit using tidymodels. `r article_req_pkgs(pkgs)`\n\n## Linear regression\n\nLet's start with a linear regression model: \n\n$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p$$ \n\nThe $\\beta$ values are the coefficients and the $x_j$ are model predictors, or features. \n\nLet's use the [Chicago train data](https://bookdown.org/max/FES/chicago-intro.html) where we predict the ridership at the Clark and Lake station (column name: `ridership`) with the previous ridership data 14 days prior at three of the stations. \n\nThe data are in the modeldata package:  \n\n```{r}\n#| label: \"setup-tm\"\n#| message: false\n#| warning: false\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n\ndata(Chicago)\n\nChicago <- Chicago %>% select(ridership, Clark_Lake, Austin, Harlem)\n```\n\n### A single model\n\nLet's start by fitting only a single parsnip model object. We'll create a model specification using `linear_reg()`. \n\n::: {.callout-note}\nThe default engine is `\"lm\"` so no call to `set_engine()` is required. \n:::\n\nThe `fit()` function estimates the model coefficients, given a formula and data set. \n\n\n```{r}\n#| label: \"lm-single\"\nlm_spec <- linear_reg()\nlm_fit <- fit(lm_spec, ridership ~ ., data = Chicago)\nlm_fit\n```\n\nThe best way to retrieve the fitted parameters is to use the `tidy()` method. This function, in the broom package, returns the coefficients and their associated statistics in a data frame with standardized column names: \n\n```{r}\n#| label: \"lm-tidy\"\ntidy(lm_fit)\n```\n\nWe'll use this function in subsequent sections. \n\n### Resampled or tuned models\n\nThe tidymodels framework emphasizes the use of resampling methods to evaluate and characterize how well a model works. While time series resampling methods are appropriate for these data, we can also use the [bootstrap](https://www.tmwr.org/resampling.html#bootstrap) to resample the data. This is a standard resampling approach when evaluating the uncertainty in statistical estimates.  \n\nWe'll use five bootstrap resamples of the data to simplify the plots and output (normally, we would use a larger number of resamples for more reliable estimates).\n\n```{r}\n#| label: \"bootstraps\"\nset.seed(123)\nbt <- bootstraps(Chicago, times = 5)\n```\n\nWith resampling, we fit the same model to the different simulated versions of the data set produced by resampling. The tidymodels function [`fit_resamples()`](https://www.tmwr.org/resampling.html#resampling-performance) is the recommended approach for doing so. \n\n::: {.callout-warning}\n The `fit_resamples()` function does not automatically save the model objects for each resample since these can be quite large and its main purpose is estimating performance. However, we can pass a function to `fit_resamples()` that _can_ save the model object or any other aspect of the fit. \n:::\n\nThis function takes a single argument that represents the fitted [workflow object](https://www.tmwr.org/workflows.html) (even if you don't give `fit_resamples()` a workflow).\n\nFrom this, we can extract the model fit. There are two \"levels\" of model objects that are available: \n\n* The parsnip model object, which wraps the underlying model object. We retrieve this using the `extract_fit_parsnip()` function. \n\n* The underlying model object (a.k.a. the engine fit) via the `extract_fit_engine()`. \n\nWe'll use the latter option and then tidy this model object as we did in the previous section. Let's add this to the control function so that we can re-use it. \n\n```{r}\n#| label: \"lm-ctrl\"\nget_lm_coefs <- function(x) {\n  x %>% \n    # get the lm model object\n    extract_fit_engine() %>% \n    # transform its format\n    tidy()\n}\ntidy_ctrl <- control_grid(extract = get_lm_coefs)\n```\n\nThis argument is then passed to `fit_resamples()`:\n\n```{r}\n#| label: \"lm-resampled\"\nlm_res <- \n  lm_spec %>% \n  fit_resamples(ridership ~ ., resamples = bt, control = tidy_ctrl)\nlm_res\n```\n\nNote that there is a `.extracts` column in our resampling results. This object contains the output of our `get_lm_coefs()` function for each resample. The structure of the elements of this column is a little complex. Let's start by looking at the first element (which corresponds to the first resample): \n\n\n```{r}\n#| label: \"lm-extract-ex\"\nlm_res$.extracts[[1]]\n```\n\nThere is _another_ column in this element called `.extracts` that has the results of the `tidy()` function call: \n\n```{r}\n#| label: \"lm-extract-again\"\nlm_res$.extracts[[1]]$.extracts[[1]]\n```\n\nThese nested columns can be flattened via the purrr `unnest()` function: \n\n```{r}\n#| label: \"lm-extract-almost\"\nlm_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) \n```\n\nWe still have a column of nested tibbles, so we can run the same command again to get the data into a more useful format: \n\n```{r}\n#| label: \"lm-extract-final\"\nlm_coefs <- \n  lm_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) %>% \n  unnest(.extracts)\n\nlm_coefs %>% select(id, term, estimate, p.value)\n```\n\nThat's better! Now, let's plot the model coefficients for each resample: \n\n```{r}\n#| label: \"lm-plot\"\nlm_coefs %>%\n  filter(term != \"(Intercept)\") %>% \n  ggplot(aes(x = term, y = estimate, group = id, col = id)) +  \n  geom_hline(yintercept = 0, lty = 3) + \n  geom_line(alpha = 0.3, lwd = 1.2) + \n  labs(y = \"Coefficient\", x = NULL) +\n  theme(legend.position = \"top\")\n```\n\nThere seems to be a lot of uncertainty in the coefficient for the Austin station data, but less for the other two. \n\nLooking at the code for unnesting the results, you may find the double-nesting structure excessive or cumbersome. However, the extraction functionality is flexible, and a simpler structure would prevent many use cases. \n\n## More complex: a glmnet model\n\nThe glmnet model can fit the same linear regression model structure shown above. It uses regularization (a.k.a penalization) to estimate the model parameters. This has the benefit of shrinking the coefficients towards zero, important in situations where there are strong correlations between predictors or if some feature selection is required. Both of these cases are true for our Chicago train data set. \n\nThere are two types of penalization that this model uses: \n\n* Lasso (a.k.a. $L_1$) penalties can shrink the model terms so much that they are absolute zero (i.e. their effect is entirely removed from the model). \n\n* Weight decay (a.k.a ridge regression or $L_2$) uses a different type of penalty that is most useful for highly correlated predictors. \n\nThe glmnet model has two primary tuning parameters, the total amount of penalization and the mixture of the two penalty types. For example, this specification:\n\n```{r}\n#| label: \"glmnet-spec\"\nglmnet_spec <- \n  linear_reg(penalty = 0.1, mixture = 0.95) %>% \n  set_engine(\"glmnet\")\n```\n\nhas a penalty that is 95% lasso and 5% weight decay. The total amount of these two penalties is 0.1 (which is fairly high). \n\n::: {.callout-note}\nModels with regularization require that predictors are all on the same scale. The ridership at our three stations are very different, but glmnet [automatically centers and scales the data](https://parsnip.tidymodels.org/reference/details_linear_reg_glmnet.html). You can use recipes to [center and scale your data yourself](https://recipes.tidymodels.org/reference/step_normalize.html). \n:::\n\nLet's combine the model specification with a formula in a model `workflow()` and then fit the model to the data:\n\n```{r}\n#| label: \"glmnet-wflow\"\nglmnet_wflow <- \n  workflow() %>% \n  add_model(glmnet_spec) %>% \n  add_formula(ridership ~ .)\n\nglmnet_fit <- fit(glmnet_wflow, Chicago)\nglmnet_fit\n```\n\nIn this output, the term `lambda` is used to represent the penalty. \n\nNote that the output shows many values of the penalty despite our specification of `penalty = 0.1`. It turns out that this model fits a \"path\" of penalty values.  Even though we are interested in a value of 0.1, we can get the model coefficients for many associated values of the penalty from the same model object. \n\nLet's look at two different approaches to obtaining the coefficients. Both will use the `tidy()` method. One will tidy a glmnet object and the other will tidy a tidymodels object. \n\n### Using glmnet penalty values\n\nThis glmnet fit contains multiple penalty values which depend on the data set; changing the data (or the mixture amount) often produces a different set of values. For this data set, there are `r length(extract_fit_engine(glmnet_fit)$lambda)` penalties available. To get the set of penalties produced for this data set, we can extract the engine fit and tidy: \n\n```{r}\n#| label: \"glmnet-tidy\"\nglmnet_fit %>% \n  extract_fit_engine() %>% \n  tidy() %>% \n  rename(penalty = lambda) %>%   # <- for consistent naming\n  filter(term != \"(Intercept)\")\n```\n\nThis works well but, it turns out that our penalty value (0.1) is not in the list produced by the model! The underlying package has functions that use interpolation to produce coefficients for this specific value, but the `tidy()` method for glmnet objects does not use it. \n\n### Using specific penalty values\n\nIf we run the `tidy()` method on the workflow or parsnip object, a different function is used that returns the coefficients for the penalty value that we specified: \n\n```{r}\n#| label: \"glmnet-tidy-parsnip\"\ntidy(glmnet_fit)\n```\n\nFor any another (single) penalty, we can use an additional argument:\n\n```{r}\n#| label: \"glmnet-tidy-parsnip-alt\"\ntidy(glmnet_fit, penalty = 5.5620)  # A value from above\n```\n\nThe reason for having two `tidy()` methods is that, with tidymodels, the focus is on using a specific penalty value. \n\n\n### Tuning a glmnet model\n\nIf we know a priori acceptable values for penalty and mixture, we can use the `fit_resamples()` function as we did before with linear regression. Otherwise, we can tune those parameters with the tidymodels `tune_*()` functions. \n\nLet's tune our glmnet model over both parameters with this grid: \n\n```{r}\n#| label: \"glmnet-grid\"\npen_vals <- 10^seq(-3, 0, length.out = 10)\ngrid <- crossing(penalty = pen_vals, mixture = c(0.1, 1.0))\n```\n\nHere is where more glmnet-related complexity comes in: we know that each resample and each value of `mixture` will probably produce a different set of penalty values contained in the model object. _How can we look at the coefficients at the specific penalty values that we are using to tune?_\n\nThe approach that we suggest is to use the special `path_values` option for glmnet. Details are described in the [technical documentation about glmnet and tidymodels](https://parsnip.tidymodels.org/reference/glmnet-details.html#arguments) but in short, this parameter will assign the collection of penalty values used by each glmnet fit (regardless of the data or value of mixture). \n\nWe can pass these as an engine argument and then update our previous workflow object:\n\n```{r}\n#| label: \"glmnet-tune\"\nglmnet_tune_spec <- \n  linear_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\", path_values = pen_vals)\n\nglmnet_wflow <- \n  glmnet_wflow %>% \n  update_model(glmnet_tune_spec)\n```\n\nNow we will use an extraction function similar to when we used ordinary least squares. We add an additional argument to retain coefficients that are shrunk to zero by the lasso penalty: \n\n```{r}\n#| label: \"glmnet-tuning\"\nget_glmnet_coefs <- function(x) {\n  x %>% \n    extract_fit_engine() %>% \n    tidy(return_zeros = TRUE) %>% \n    rename(penalty = lambda)\n}\nparsnip_ctrl <- control_grid(extract = get_glmnet_coefs)\n\nglmnet_res <- \n  glmnet_wflow %>% \n  tune_grid(\n    resamples = bt,\n    grid = grid,\n    control = parsnip_ctrl\n  )\nglmnet_res\n```\n\nAs noted before, the elements of the main `.extracts` column have an embedded list column with the results of `get_glmnet_coefs()`:  \n\n```{r}\n#| label: \"glmnet-extract-single\"\nglmnet_res$.extracts[[1]] %>% head()\n\nglmnet_res$.extracts[[1]]$.extracts[[1]] %>% head()\n```\n\nAs before, we'll have to use a double `unnest()`. Since the penalty value is in both the top-level and lower-level `.extracts`, we'll use `select()` to get rid of the first version (but keep `mixture`):\n\n```{r}\n#| label: \"glmnet-extract-1\"\n#| eval: false\nglmnet_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) %>% \n  select(id, mixture, .extracts) %>%  # <- removes the first penalty column\n  unnest(.extracts)\n```\n\nBut wait! We know that each glmnet fit contains all of the coefficients. This means, for a specific resample and value of `mixture`, the results are the same:  \n\n```{r}\n#| label: \"glmnet-extract-dups\"\nall.equal(\n  # First bootstrap, first `mixture`, first `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[1]],\n  # First bootstrap, first `mixture`, second `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[2]]\n)\n```\n\nFor this reason, we'll add a `slice(1)` when grouping by `id` and `mixture`. This will get rid of the replicated results. \n\n```{r}\n#| label: \"glmnet-extract-final\"\nglmnet_coefs <- \n  glmnet_res %>% \n  select(id, .extracts) %>% \n  unnest(.extracts) %>% \n  select(id, mixture, .extracts) %>% \n  group_by(id, mixture) %>%          # ┐\n  slice(1) %>%                       # │ Remove the redundant results\n  ungroup() %>%                      # ┘\n  unnest(.extracts)\n\nglmnet_coefs %>% \n  select(id, penalty, mixture, term, estimate) %>% \n  filter(term != \"(Intercept)\")\n```\n\nNow we have the coefficients. Let's look at how they behave as more regularization is used: \n\n```{r}\n#| label: \"glmnet-plot\"\n#| fig-height:  4\n#| fig-width:  8.5\nglmnet_coefs %>% \n  filter(term != \"(Intercept)\") %>% \n  mutate(mixture = format(mixture)) %>% \n  ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) + \n  geom_hline(yintercept = 0, lty = 3) +\n  geom_line(alpha = 0.5, lwd = 1.2) + \n  facet_wrap(~ term) + \n  scale_x_log10() +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(y = \"coefficient\") +\n  theme(legend.position = \"top\")\n```\n\nNotice a couple of things: \n\n* With a pure lasso model (i.e., `mixture = 1`), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases. Also, as the penalty increases, the uncertainty in this coefficient decreases. \n\n* The Harlem predictor is either quickly selected out of the model or goes from negative to positive. \n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"include-after-body":["../../../resources.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","notebook-preview-download":"Download Notebook","notebook-preview-back":"Back to Article"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.92","theme":["cosmo","../../../styles.scss","../../../styles-frontpage.scss"],"quarto-required":">= 1.3.353","linestretch":1.6,"grid":{"body-width":"840px"},"title":"Working with model coefficients","categories":["model fitting","tidying results","linear regression","model tuning"],"type":"learn-subsection","weight":5,"description":"Create models that use coefficients, extract them from fitted models, and visualize them.\n"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}