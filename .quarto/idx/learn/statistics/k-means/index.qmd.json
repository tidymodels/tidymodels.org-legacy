{"title":"K-means clustering with tidy data principles","markdown":{"yaml":{"title":"K-means clustering with tidy data principles","categories":["statistical analysis","clustering","tidying results"],"type":"learn-subsection","weight":2,"description":"Summarize clustering characteristics and estimate the best number of clusters for a data set.\n","toc":true,"toc-depth":2,"include-after-body":"../../../resources.html"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n\n```{r}\n#| label: \"load\"\n#| include: false\nlibrary(tidymodels)\npkgs <- c(\"tidymodels\")\n\ntheme_set(theme_bw() + theme(legend.position = \"top\"))\n```\n\n\nThis article only requires the tidymodels package.\n\nK-means clustering serves as a useful example of applying tidy data principles to statistical analysis, and especially the distinction between the three tidying functions: \n\n- `tidy()`\n- `augment()` \n- `glance()`\n\nLet's start by generating some random two-dimensional data with three clusters. Data in each cluster will come from a multivariate gaussian distribution, with different means for each cluster:\n\n```{r}\nlibrary(tidymodels)\n\nset.seed(27)\n\ncenters <- tibble(\n  cluster = factor(1:3), \n  num_points = c(100, 150, 50),  # number points in each cluster\n  x1 = c(5, 0, -3),              # x1 coordinate of cluster center\n  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center\n)\n\nlabelled_points <- \n  centers %>%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) %>% \n  select(-num_points) %>% \n  unnest(cols = c(x1, x2))\n\nggplot(labelled_points, aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3)\n```\n\nThis is an ideal case for k-means clustering. \n\n## How does K-means work?\n\nRather than using equations, this short animation using the [artwork](https://github.com/allisonhorst/stats-illustrations) of Allison Horst explains the clustering process:\n\n```{r}\n#| label: \"illustrations\"\n#| echo: false\n#| results: asis\n#| fig-align: center\nknitr::include_graphics(\"kmeans.gif\")\n```\n\n## Clustering in R\n\nWe'll use the built-in `kmeans()` function, which accepts a data frame with all numeric columns as it's primary argument.\n\n```{r}\npoints <- \n  labelled_points %>% \n  select(-cluster)\n\nkclust <- kmeans(points, centers = 3)\nkclust\nsummary(kclust)\n```\n\nThe output is a list of vectors, where each component has a different length. There's one of length `r nrow(points)`, the same as our original data set. There are two elements of length 3 (`withinss` and `tot.withinss`) and `centers` is a matrix with 3 rows. And then there are the elements of length 1: `totss`, `tot.withinss`, `betweenss`, and `iter`. (The value `ifault` indicates possible algorithm problems.)\n\nThese differing lengths have important meaning when we want to tidy our data set; they signify that each type of component communicates a *different kind* of information.\n\n- `cluster` (`r nrow(points)` values) contains information about each *point*\n- `centers`, `withinss`, and `size` (3 values) contain information about each *cluster*\n- `totss`, `tot.withinss`, `betweenss`, and `iter` (1 value) contain information about the *full clustering*\n\nWhich of these do we want to extract? There is no right answer; each of them may be interesting to an analyst. Because they communicate entirely different information (not to mention there's no straightforward way to combine them), they are extracted by separate functions. `augment` adds the point classifications to the original data set:\n\n```{r}\naugment(kclust, points)\n```\n\nThe `tidy()` function summarizes on a per-cluster level:\n\n```{r}\ntidy(kclust)\n```\n\nAnd as it always does, the `glance()` function extracts a single-row summary:\n\n```{r}\nglance(kclust)\n```\n\n## Exploratory clustering\n\nWhile these summaries are useful, they would not have been too difficult to extract out from the data set yourself. The real power comes from combining these analyses with other tools like [dplyr](https://dplyr.tidyverse.org/).\n\nLet's say we want to explore the effect of different choices of `k`, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of `k`, then create columns containing the tidied, glanced and augmented data:\n\n```{r}\nkclusts <- \n  tibble(k = 1:9) %>%\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, points)\n  )\n\nkclusts\n```\n\nWe can turn these into three separate data sets each representing a different type of data: using `tidy()`, using `augment()`, and using `glance()`. Each of these goes into a separate data set as they represent different types of data.\n\n```{r}\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\n```\n\nNow we can plot the original points using the data from `augment()`, with each point colored according to the predicted cluster.\n\n```{r}\n#| fig-width:  7\n#| fig-height:  7\np1 <- \n  ggplot(assignments, aes(x = x1, y = x2)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) + \n  facet_wrap(~ k)\np1\n```\n\nAlready we get a good sense of the proper number of clusters (3), and how the k-means algorithm functions when `k` is too high or too low. We can then add the centers of the cluster using the data from `tidy()`:\n\n```{r}\np2 <- p1 + geom_point(data = clusters, size = 10, shape = \"x\")\np2\n```\n\nThe data from `glance()` fills a different but equally important purpose; it lets us view trends of some summary statistics across values of `k`. Of particular interest is the total within sum of squares, saved in the `tot.withinss` column.\n\n```{r}\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n```\n\nThis represents the variance within the clusters. It decreases as `k` increases, but notice a bend (or \"elbow\") around `k = 3`. This bend indicates that additional clusters beyond the third have little value. (See [here](https://web.stanford.edu/~hastie/Papers/gap.pdf) for a more mathematically rigorous interpretation and implementation of this method). Thus, all three methods of tidying data provided by broom are useful for summarizing clustering output.\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n\n```{r}\n#| label: \"load\"\n#| include: false\nlibrary(tidymodels)\npkgs <- c(\"tidymodels\")\n\ntheme_set(theme_bw() + theme(legend.position = \"top\"))\n```\n\n## Introduction\n\nThis article only requires the tidymodels package.\n\nK-means clustering serves as a useful example of applying tidy data principles to statistical analysis, and especially the distinction between the three tidying functions: \n\n- `tidy()`\n- `augment()` \n- `glance()`\n\nLet's start by generating some random two-dimensional data with three clusters. Data in each cluster will come from a multivariate gaussian distribution, with different means for each cluster:\n\n```{r}\nlibrary(tidymodels)\n\nset.seed(27)\n\ncenters <- tibble(\n  cluster = factor(1:3), \n  num_points = c(100, 150, 50),  # number points in each cluster\n  x1 = c(5, 0, -3),              # x1 coordinate of cluster center\n  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center\n)\n\nlabelled_points <- \n  centers %>%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) %>% \n  select(-num_points) %>% \n  unnest(cols = c(x1, x2))\n\nggplot(labelled_points, aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3)\n```\n\nThis is an ideal case for k-means clustering. \n\n## How does K-means work?\n\nRather than using equations, this short animation using the [artwork](https://github.com/allisonhorst/stats-illustrations) of Allison Horst explains the clustering process:\n\n```{r}\n#| label: \"illustrations\"\n#| echo: false\n#| results: asis\n#| fig-align: center\nknitr::include_graphics(\"kmeans.gif\")\n```\n\n## Clustering in R\n\nWe'll use the built-in `kmeans()` function, which accepts a data frame with all numeric columns as it's primary argument.\n\n```{r}\npoints <- \n  labelled_points %>% \n  select(-cluster)\n\nkclust <- kmeans(points, centers = 3)\nkclust\nsummary(kclust)\n```\n\nThe output is a list of vectors, where each component has a different length. There's one of length `r nrow(points)`, the same as our original data set. There are two elements of length 3 (`withinss` and `tot.withinss`) and `centers` is a matrix with 3 rows. And then there are the elements of length 1: `totss`, `tot.withinss`, `betweenss`, and `iter`. (The value `ifault` indicates possible algorithm problems.)\n\nThese differing lengths have important meaning when we want to tidy our data set; they signify that each type of component communicates a *different kind* of information.\n\n- `cluster` (`r nrow(points)` values) contains information about each *point*\n- `centers`, `withinss`, and `size` (3 values) contain information about each *cluster*\n- `totss`, `tot.withinss`, `betweenss`, and `iter` (1 value) contain information about the *full clustering*\n\nWhich of these do we want to extract? There is no right answer; each of them may be interesting to an analyst. Because they communicate entirely different information (not to mention there's no straightforward way to combine them), they are extracted by separate functions. `augment` adds the point classifications to the original data set:\n\n```{r}\naugment(kclust, points)\n```\n\nThe `tidy()` function summarizes on a per-cluster level:\n\n```{r}\ntidy(kclust)\n```\n\nAnd as it always does, the `glance()` function extracts a single-row summary:\n\n```{r}\nglance(kclust)\n```\n\n## Exploratory clustering\n\nWhile these summaries are useful, they would not have been too difficult to extract out from the data set yourself. The real power comes from combining these analyses with other tools like [dplyr](https://dplyr.tidyverse.org/).\n\nLet's say we want to explore the effect of different choices of `k`, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of `k`, then create columns containing the tidied, glanced and augmented data:\n\n```{r}\nkclusts <- \n  tibble(k = 1:9) %>%\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, points)\n  )\n\nkclusts\n```\n\nWe can turn these into three separate data sets each representing a different type of data: using `tidy()`, using `augment()`, and using `glance()`. Each of these goes into a separate data set as they represent different types of data.\n\n```{r}\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\n```\n\nNow we can plot the original points using the data from `augment()`, with each point colored according to the predicted cluster.\n\n```{r}\n#| fig-width:  7\n#| fig-height:  7\np1 <- \n  ggplot(assignments, aes(x = x1, y = x2)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) + \n  facet_wrap(~ k)\np1\n```\n\nAlready we get a good sense of the proper number of clusters (3), and how the k-means algorithm functions when `k` is too high or too low. We can then add the centers of the cluster using the data from `tidy()`:\n\n```{r}\np2 <- p1 + geom_point(data = clusters, size = 10, shape = \"x\")\np2\n```\n\nThe data from `glance()` fills a different but equally important purpose; it lets us view trends of some summary statistics across values of `k`. Of particular interest is the total within sum of squares, saved in the `tot.withinss` column.\n\n```{r}\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n```\n\nThis represents the variance within the clusters. It decreases as `k` increases, but notice a bend (or \"elbow\") around `k = 3`. This bend indicates that additional clusters beyond the third have little value. (See [here](https://web.stanford.edu/~hastie/Papers/gap.pdf) for a more mathematically rigorous interpretation and implementation of this method). Thus, all three methods of tidying data provided by broom are useful for summarizing clustering output.\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"include-after-body":["../../../resources.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","notebook-preview-download":"Download Notebook","notebook-preview-back":"Back to Article"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.92","theme":["cosmo","../../../styles.scss","../../../styles-frontpage.scss"],"quarto-required":">= 1.3.353","linestretch":1.6,"grid":{"body-width":"840px"},"title":"K-means clustering with tidy data principles","categories":["statistical analysis","clustering","tidying results"],"type":"learn-subsection","weight":2,"description":"Summarize clustering characteristics and estimate the best number of clusters for a data set.\n"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}