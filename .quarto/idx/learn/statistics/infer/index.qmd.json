{"title":"Hypothesis testing using resampling and tidy data","markdown":{"yaml":{"title":"Hypothesis testing using resampling and tidy data","categories":["statistical analysis","hypothesis testing","bootstraping"],"type":"learn-subsection","weight":4,"description":"Perform common hypothesis tests for statistical inference using flexible functions.\n","toc":true,"toc-depth":2,"include-after-body":"../../../resources.html"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n\n```{r}\n#| label: \"load\"\n#| include: false\nlibrary(tidymodels)\nlibrary(sessioninfo)\npkgs <- c(\"tidymodels\")\ntheme_set(theme_bw() + theme(legend.position = \"top\"))\n```\n\n\nThis article only requires the tidymodels package. \n\nThe tidymodels package [infer](https://infer.tidymodels.org/) implements an expressive grammar to perform statistical inference that coheres with the `tidyverse` design framework. Rather than providing methods for specific statistical tests, this package consolidates the principles that are shared among common hypothesis tests into a set of 4 main verbs (functions), supplemented with many utilities to visualize and extract information from their outputs.\n\nRegardless of which hypothesis test we're using, we're still asking the same kind of question: \n\n>Is the effect or difference in our observed data real, or due to chance? \n\nTo answer this question, we start by assuming that the observed data came from some world where \"nothing is going on\" (i.e. the observed effect was simply due to random chance), and call this assumption our **null hypothesis**. (In reality, we might not believe in the null hypothesis at all; the null hypothesis is in opposition to the **alternate hypothesis**, which supposes that the effect present in the observed data is actually due to the fact that \"something is going on.\") We then calculate a **test statistic** from our data that describes the observed effect. We can use this test statistic to calculate a **p-value**, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined **significance level** $\\alpha$, then we can reject our null hypothesis.\n\nIf you are new to hypothesis testing, take a look at \n\n* [Section 9.2 of _Statistical Inference via Data Science_](https://moderndive.com/9-hypothesis-testing.html#understanding-ht)\n* The American Statistical Association's recent [statement on p-values](https://doi.org/10.1080/00031305.2016.1154108) \n\nThe workflow of this package is designed around these ideas. Starting from some data set,\n\n+ `specify()` allows you to specify the variable, or relationship between variables, that you're interested in,\n+ `hypothesize()` allows you to declare the null hypothesis,\n+ `generate()` allows you to generate data reflecting the null hypothesis, and\n+ `calculate()` allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThroughout this vignette, we make use of `gss`, a data set available in infer containing a sample of 500 observations of 11 variables from the *General Social Survey*. \n\n```{r}\n#| label: \"load-gss\"\n#| warning: false\n#| message: false\nlibrary(tidymodels) # Includes the infer package\n\n# load in the data set\ndata(gss)\n\n# take a look at its structure\ndplyr::glimpse(gss)\n```\n\nEach row is an individual survey response, containing some basic demographic information on the respondent as well as some additional variables. See `?gss` for more information on the variables included and their source. Note that this data (and our examples on it) are for demonstration purposes only, and will not necessarily provide accurate estimates unless weighted properly. For these examples, let's suppose that this data set is a representative sample of a population we want to learn about: American adults.\n\n## Specify variables\n\nThe `specify()` function can be used to specify which of the variables in the data set you're interested in. If you're only interested in, say, the `age` of the respondents, you might write:\n\n```{r}\n#| label: \"specify-example\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(response = age)\n```\n\nOn the front end, the output of `specify()` just looks like it selects off the columns in the dataframe that you've specified. What do we see if we check the class of this object, though?\n\n```{r}\n#| label: \"specify-one\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(response = age) %>%\n  class()\n```\n\nWe can see that the infer class has been appended on top of the dataframe classes; this new class stores some extra metadata.\n\nIf you're interested in two variables (`age` and `partyid`, for example) you can `specify()` their relationship in one of two (equivalent) ways:\n\n```{r}\n#| label: \"specify-two\"\n#| warning: false\n#| message: false\n# as a formula\ngss %>%\n  specify(age ~ partyid)\n\n# with the named arguments\ngss %>%\n  specify(response = age, explanatory = partyid)\n```\n\nIf you're doing inference on one proportion or a difference in proportions, you will need to use the `success` argument to specify which level of your `response` variable is a success. For instance, if you're interested in the proportion of the population with a college degree, you might use the following code:\n\n```{r}\n#| label: \"specify-success\"\n#| warning: false\n#| message: false\n# specifying for inference on proportions\ngss %>%\n  specify(response = college, success = \"degree\")\n```\n\n## Declare the hypothesis\n\nThe next step in the infer pipeline is often to declare a null hypothesis using `hypothesize()`. The first step is to supply one of \"independence\" or \"point\" to the `null` argument. If your null hypothesis assumes independence between two variables, then this is all you need to supply to `hypothesize()`:\n\n```{r}\n#| label: \"hypothesize-independence\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(college ~ partyid, success = \"degree\") %>%\n  hypothesize(null = \"independence\")\n```\n\nIf you're doing inference on a point estimate, you will also need to provide one of `p` (the true proportion of successes, between 0 and 1), `mu` (the true mean), `med` (the true median), or `sigma` (the true standard deviation). For instance, if the null hypothesis is that the mean number of hours worked per week in our population is 40, we would write:\n\n```{r}\n#| label: \"hypothesize-40-hr-week\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40)\n```\n\nAgain, from the front-end, the dataframe outputted from `hypothesize()` looks almost exactly the same as it did when it came out of `specify()`, but infer now \"knows\" your null hypothesis.\n\n## Generate the distribution\n\nOnce we've asserted our null hypothesis using `hypothesize()`, we can construct a null distribution based on this hypothesis. We can do this using one of several methods, supplied in the `type` argument:\n\n* `bootstrap`: A bootstrap sample will be drawn for each replicate, where a sample of size equal to the input sample size is drawn (with replacement) from the input sample data.  \n* `permute`: For each replicate, each input value will be randomly reassigned (without replacement) to a new output value in the sample.  \n* `simulate`: A value will be sampled from a theoretical distribution with parameters specified in `hypothesize()` for each replicate. (This option is currently only applicable for testing point estimates.)  \n\nContinuing on with our example above, about the average number of hours worked a week, we might write:\n\n```{r}\n#| label: \"generate-point\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40) %>%\n  generate(reps = 5000, type = \"bootstrap\")\n```\n\nIn the above example, we take 5000 bootstrap samples to form our null distribution.\n\nTo generate a null distribution for the independence of two variables, we could also randomly reshuffle the pairings of explanatory and response variables to break any existing association. For instance, to generate 5000 replicates that can be used to create a null distribution under the assumption that political party affiliation is not affected by age:\n\n```{r}\n#| label: \"generate-permute\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(partyid ~ age) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 5000, type = \"permute\")\n```\n\n## Calculate statistics\n\nDepending on whether you're carrying out computation-based inference or theory-based inference, you will either supply `calculate()` with the output of `generate()` or `hypothesize()`, respectively. The function, for one, takes in a `stat` argument, which is currently one of `\"mean\"`, `\"median\"`, `\"sum\"`, `\"sd\"`, `\"prop\"`, `\"count\"`, `\"diff in means\"`, `\"diff in medians\"`, `\"diff in props\"`, `\"Chisq\"`, `\"F\"`, `\"t\"`, `\"z\"`, `\"slope\"`, or `\"correlation\"`. For example, continuing our example above to calculate the null distribution of mean hours worked per week:\n\n```{r}\n#| label: \"calculate-point\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40) %>%\n  generate(reps = 5000, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n```\n\nThe output of `calculate()` here shows us the sample statistic (in this case, the mean) for each of our 1000 replicates. If you're carrying out inference on differences in means, medians, or proportions, or $t$ and $z$ statistics, you will need to supply an `order` argument, giving the order in which the explanatory variables should be subtracted. For instance, to find the difference in mean age of those that have a college degree and those that don't, we might write:\n\n```{r}\n#| label: \"specify-diff-in-means\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(age ~ college) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 5000, type = \"permute\") %>%\n  calculate(\"diff in means\", order = c(\"degree\", \"no degree\"))\n```\n\n## Other utilities\n\nThe infer package also offers several utilities to extract meaning out of summary statistics and null distributions; the package provides functions to visualize where a statistic is relative to a distribution (with `visualize()`), calculate p-values (with `get_p_value()`), and calculate confidence intervals (with `get_confidence_interval()`).\n\nTo illustrate, we'll go back to the example of determining whether the mean number of hours worked per week is 40 hours.\n\n```{r}\n#| label: \"utilities-examples\"\n# find the point estimate\npoint_estimate <- gss %>%\n  specify(response = hours) %>%\n  calculate(stat = \"mean\")\n\n# generate a null distribution\nnull_dist <- gss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40) %>%\n  generate(reps = 5000, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n```\n\n(Notice the warning: `Removed 1244 rows containing missing values.` This would be worth noting if you were actually carrying out this hypothesis test.)\n\nOur point estimate `r point_estimate` seems *pretty* close to 40, but a little bit different. We might wonder if this difference is just due to random chance, or if the mean number of hours worked per week in the population really isn't 40.\n\nWe could initially just visualize the null distribution.\n\n```{r}\n#| label: \"visualize\"\n#| warning: false\n#| message: false\nnull_dist %>%\n  visualize()\n```\n\nWhere does our sample's observed statistic lie on this distribution? We can use the `obs_stat` argument to specify this.\n\n```{r}\n#| label: \"visualize2\"\n#| warning: false\n#| message: false\nnull_dist %>%\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two_sided\")\n```\n\nNotice that infer has also shaded the regions of the null distribution that are as (or more) extreme than our observed statistic. (Also, note that we now use the `+` operator to apply the `shade_p_value()` function. This is because `visualize()` outputs a plot object from ggplot2 instead of a dataframe, and the `+` operator is needed to add the p-value layer to the plot object.) The red bar looks like it's slightly far out on the right tail of the null distribution, so observing a sample mean of `r point_estimate` hours would be somewhat unlikely if the mean was actually 40 hours. How unlikely, though?\n\n```{r}\n#| label: \"get_p_value\"\n#| warning: false\n#| message: false\n# get a two-tailed p-value\np_value <- null_dist %>%\n  get_p_value(obs_stat = point_estimate, direction = \"two_sided\")\n\np_value\n```\n\nIt looks like the p-value is `r p_value`, which is pretty small---if the true mean number of hours worked per week was actually 40, the probability of our sample mean being this far (`r abs(point_estimate-40)` hours) from 40 would be `r p_value`. This may or may not be statistically significantly different, depending on the significance level $\\alpha$ you decided on *before* you ran this analysis. If you had set $\\alpha = .05$, then this difference would be statistically significant, but if you had set $\\alpha = .01$, then it would not be.\n\nTo get a confidence interval around our estimate, we can write:\n\n```{r}\n#| label: \"get_conf\"\n#| message: false\n#| warning: false\n# start with the null distribution\nnull_dist %>%\n  # calculate the confidence interval around the point estimate\n  get_confidence_interval(point_estimate = point_estimate,\n                          # at the 95% confidence level\n                          level = .95,\n                          # using the standard error\n                          type = \"se\")\n```\n\nAs you can see, 40 hours per week is not contained in this interval, which aligns with our previous conclusion that this finding is significant at the confidence level $\\alpha = .05$.\n\n## Theoretical methods\n\nThe infer package also provides functionality to use theoretical methods for `\"Chisq\"`, `\"F\"` and `\"t\"` test statistics. \n\nGenerally, to find a null distribution using theory-based methods, use the same code that you would use to find the null distribution using randomization-based methods, but skip the `generate()` step. For example, if we wanted to find a null distribution for the relationship between age (`age`) and party identification (`partyid`) using randomization, we could write:\n\n```{r}\n#| message: false\n#| warning: false\nnull_f_distn <- gss %>%\n   specify(age ~ partyid) %>%\n   hypothesize(null = \"independence\") %>%\n   generate(reps = 5000, type = \"permute\") %>%\n   calculate(stat = \"F\")\n```\n\nTo find the null distribution using theory-based methods, instead, skip the `generate()` step entirely:\n\n```{r}\n#| message: false\n#| warning: false\nnull_f_distn_theoretical <- gss %>%\n   specify(age ~ partyid) %>%\n   hypothesize(null = \"independence\") %>%\n   calculate(stat = \"F\")\n```\n\nWe'll calculate the observed statistic to make use of in the following visualizations; this procedure is the same, regardless of the methods used to find the null distribution.\n\n```{r}\n#| message: false\n#| warning: false\nF_hat <- gss %>% \n  specify(age ~ partyid) %>%\n  calculate(stat = \"F\")\n```\n\nNow, instead of just piping the null distribution into `visualize()`, as we would do if we wanted to visualize the randomization-based null distribution, we also need to provide `method = \"theoretical\"` to `visualize()`.\n\n```{r}\n#| message: false\n#| warning: false\nvisualize(null_f_distn_theoretical, method = \"theoretical\") +\n  shade_p_value(obs_stat = F_hat, direction = \"greater\")\n```\n\nTo get a sense of how the theory-based and randomization-based null distributions relate, we can pipe the randomization-based null distribution into `visualize()` and also specify `method = \"both\"`\n\n```{r}\n#| message: false\n#| warning: false\nvisualize(null_f_distn, method = \"both\") +\n  shade_p_value(obs_stat = F_hat, direction = \"greater\")\n```\n\nThat's it! This vignette covers most all of the key functionality of infer. See `help(package = \"infer\")` for a full list of functions and vignettes.\n\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n \n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n\n```{r}\n#| label: \"load\"\n#| include: false\nlibrary(tidymodels)\nlibrary(sessioninfo)\npkgs <- c(\"tidymodels\")\ntheme_set(theme_bw() + theme(legend.position = \"top\"))\n```\n\n## Introduction\n\nThis article only requires the tidymodels package. \n\nThe tidymodels package [infer](https://infer.tidymodels.org/) implements an expressive grammar to perform statistical inference that coheres with the `tidyverse` design framework. Rather than providing methods for specific statistical tests, this package consolidates the principles that are shared among common hypothesis tests into a set of 4 main verbs (functions), supplemented with many utilities to visualize and extract information from their outputs.\n\nRegardless of which hypothesis test we're using, we're still asking the same kind of question: \n\n>Is the effect or difference in our observed data real, or due to chance? \n\nTo answer this question, we start by assuming that the observed data came from some world where \"nothing is going on\" (i.e. the observed effect was simply due to random chance), and call this assumption our **null hypothesis**. (In reality, we might not believe in the null hypothesis at all; the null hypothesis is in opposition to the **alternate hypothesis**, which supposes that the effect present in the observed data is actually due to the fact that \"something is going on.\") We then calculate a **test statistic** from our data that describes the observed effect. We can use this test statistic to calculate a **p-value**, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined **significance level** $\\alpha$, then we can reject our null hypothesis.\n\nIf you are new to hypothesis testing, take a look at \n\n* [Section 9.2 of _Statistical Inference via Data Science_](https://moderndive.com/9-hypothesis-testing.html#understanding-ht)\n* The American Statistical Association's recent [statement on p-values](https://doi.org/10.1080/00031305.2016.1154108) \n\nThe workflow of this package is designed around these ideas. Starting from some data set,\n\n+ `specify()` allows you to specify the variable, or relationship between variables, that you're interested in,\n+ `hypothesize()` allows you to declare the null hypothesis,\n+ `generate()` allows you to generate data reflecting the null hypothesis, and\n+ `calculate()` allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThroughout this vignette, we make use of `gss`, a data set available in infer containing a sample of 500 observations of 11 variables from the *General Social Survey*. \n\n```{r}\n#| label: \"load-gss\"\n#| warning: false\n#| message: false\nlibrary(tidymodels) # Includes the infer package\n\n# load in the data set\ndata(gss)\n\n# take a look at its structure\ndplyr::glimpse(gss)\n```\n\nEach row is an individual survey response, containing some basic demographic information on the respondent as well as some additional variables. See `?gss` for more information on the variables included and their source. Note that this data (and our examples on it) are for demonstration purposes only, and will not necessarily provide accurate estimates unless weighted properly. For these examples, let's suppose that this data set is a representative sample of a population we want to learn about: American adults.\n\n## Specify variables\n\nThe `specify()` function can be used to specify which of the variables in the data set you're interested in. If you're only interested in, say, the `age` of the respondents, you might write:\n\n```{r}\n#| label: \"specify-example\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(response = age)\n```\n\nOn the front end, the output of `specify()` just looks like it selects off the columns in the dataframe that you've specified. What do we see if we check the class of this object, though?\n\n```{r}\n#| label: \"specify-one\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(response = age) %>%\n  class()\n```\n\nWe can see that the infer class has been appended on top of the dataframe classes; this new class stores some extra metadata.\n\nIf you're interested in two variables (`age` and `partyid`, for example) you can `specify()` their relationship in one of two (equivalent) ways:\n\n```{r}\n#| label: \"specify-two\"\n#| warning: false\n#| message: false\n# as a formula\ngss %>%\n  specify(age ~ partyid)\n\n# with the named arguments\ngss %>%\n  specify(response = age, explanatory = partyid)\n```\n\nIf you're doing inference on one proportion or a difference in proportions, you will need to use the `success` argument to specify which level of your `response` variable is a success. For instance, if you're interested in the proportion of the population with a college degree, you might use the following code:\n\n```{r}\n#| label: \"specify-success\"\n#| warning: false\n#| message: false\n# specifying for inference on proportions\ngss %>%\n  specify(response = college, success = \"degree\")\n```\n\n## Declare the hypothesis\n\nThe next step in the infer pipeline is often to declare a null hypothesis using `hypothesize()`. The first step is to supply one of \"independence\" or \"point\" to the `null` argument. If your null hypothesis assumes independence between two variables, then this is all you need to supply to `hypothesize()`:\n\n```{r}\n#| label: \"hypothesize-independence\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(college ~ partyid, success = \"degree\") %>%\n  hypothesize(null = \"independence\")\n```\n\nIf you're doing inference on a point estimate, you will also need to provide one of `p` (the true proportion of successes, between 0 and 1), `mu` (the true mean), `med` (the true median), or `sigma` (the true standard deviation). For instance, if the null hypothesis is that the mean number of hours worked per week in our population is 40, we would write:\n\n```{r}\n#| label: \"hypothesize-40-hr-week\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40)\n```\n\nAgain, from the front-end, the dataframe outputted from `hypothesize()` looks almost exactly the same as it did when it came out of `specify()`, but infer now \"knows\" your null hypothesis.\n\n## Generate the distribution\n\nOnce we've asserted our null hypothesis using `hypothesize()`, we can construct a null distribution based on this hypothesis. We can do this using one of several methods, supplied in the `type` argument:\n\n* `bootstrap`: A bootstrap sample will be drawn for each replicate, where a sample of size equal to the input sample size is drawn (with replacement) from the input sample data.  \n* `permute`: For each replicate, each input value will be randomly reassigned (without replacement) to a new output value in the sample.  \n* `simulate`: A value will be sampled from a theoretical distribution with parameters specified in `hypothesize()` for each replicate. (This option is currently only applicable for testing point estimates.)  \n\nContinuing on with our example above, about the average number of hours worked a week, we might write:\n\n```{r}\n#| label: \"generate-point\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40) %>%\n  generate(reps = 5000, type = \"bootstrap\")\n```\n\nIn the above example, we take 5000 bootstrap samples to form our null distribution.\n\nTo generate a null distribution for the independence of two variables, we could also randomly reshuffle the pairings of explanatory and response variables to break any existing association. For instance, to generate 5000 replicates that can be used to create a null distribution under the assumption that political party affiliation is not affected by age:\n\n```{r}\n#| label: \"generate-permute\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(partyid ~ age) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 5000, type = \"permute\")\n```\n\n## Calculate statistics\n\nDepending on whether you're carrying out computation-based inference or theory-based inference, you will either supply `calculate()` with the output of `generate()` or `hypothesize()`, respectively. The function, for one, takes in a `stat` argument, which is currently one of `\"mean\"`, `\"median\"`, `\"sum\"`, `\"sd\"`, `\"prop\"`, `\"count\"`, `\"diff in means\"`, `\"diff in medians\"`, `\"diff in props\"`, `\"Chisq\"`, `\"F\"`, `\"t\"`, `\"z\"`, `\"slope\"`, or `\"correlation\"`. For example, continuing our example above to calculate the null distribution of mean hours worked per week:\n\n```{r}\n#| label: \"calculate-point\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40) %>%\n  generate(reps = 5000, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n```\n\nThe output of `calculate()` here shows us the sample statistic (in this case, the mean) for each of our 1000 replicates. If you're carrying out inference on differences in means, medians, or proportions, or $t$ and $z$ statistics, you will need to supply an `order` argument, giving the order in which the explanatory variables should be subtracted. For instance, to find the difference in mean age of those that have a college degree and those that don't, we might write:\n\n```{r}\n#| label: \"specify-diff-in-means\"\n#| warning: false\n#| message: false\ngss %>%\n  specify(age ~ college) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 5000, type = \"permute\") %>%\n  calculate(\"diff in means\", order = c(\"degree\", \"no degree\"))\n```\n\n## Other utilities\n\nThe infer package also offers several utilities to extract meaning out of summary statistics and null distributions; the package provides functions to visualize where a statistic is relative to a distribution (with `visualize()`), calculate p-values (with `get_p_value()`), and calculate confidence intervals (with `get_confidence_interval()`).\n\nTo illustrate, we'll go back to the example of determining whether the mean number of hours worked per week is 40 hours.\n\n```{r}\n#| label: \"utilities-examples\"\n# find the point estimate\npoint_estimate <- gss %>%\n  specify(response = hours) %>%\n  calculate(stat = \"mean\")\n\n# generate a null distribution\nnull_dist <- gss %>%\n  specify(response = hours) %>%\n  hypothesize(null = \"point\", mu = 40) %>%\n  generate(reps = 5000, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n```\n\n(Notice the warning: `Removed 1244 rows containing missing values.` This would be worth noting if you were actually carrying out this hypothesis test.)\n\nOur point estimate `r point_estimate` seems *pretty* close to 40, but a little bit different. We might wonder if this difference is just due to random chance, or if the mean number of hours worked per week in the population really isn't 40.\n\nWe could initially just visualize the null distribution.\n\n```{r}\n#| label: \"visualize\"\n#| warning: false\n#| message: false\nnull_dist %>%\n  visualize()\n```\n\nWhere does our sample's observed statistic lie on this distribution? We can use the `obs_stat` argument to specify this.\n\n```{r}\n#| label: \"visualize2\"\n#| warning: false\n#| message: false\nnull_dist %>%\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two_sided\")\n```\n\nNotice that infer has also shaded the regions of the null distribution that are as (or more) extreme than our observed statistic. (Also, note that we now use the `+` operator to apply the `shade_p_value()` function. This is because `visualize()` outputs a plot object from ggplot2 instead of a dataframe, and the `+` operator is needed to add the p-value layer to the plot object.) The red bar looks like it's slightly far out on the right tail of the null distribution, so observing a sample mean of `r point_estimate` hours would be somewhat unlikely if the mean was actually 40 hours. How unlikely, though?\n\n```{r}\n#| label: \"get_p_value\"\n#| warning: false\n#| message: false\n# get a two-tailed p-value\np_value <- null_dist %>%\n  get_p_value(obs_stat = point_estimate, direction = \"two_sided\")\n\np_value\n```\n\nIt looks like the p-value is `r p_value`, which is pretty small---if the true mean number of hours worked per week was actually 40, the probability of our sample mean being this far (`r abs(point_estimate-40)` hours) from 40 would be `r p_value`. This may or may not be statistically significantly different, depending on the significance level $\\alpha$ you decided on *before* you ran this analysis. If you had set $\\alpha = .05$, then this difference would be statistically significant, but if you had set $\\alpha = .01$, then it would not be.\n\nTo get a confidence interval around our estimate, we can write:\n\n```{r}\n#| label: \"get_conf\"\n#| message: false\n#| warning: false\n# start with the null distribution\nnull_dist %>%\n  # calculate the confidence interval around the point estimate\n  get_confidence_interval(point_estimate = point_estimate,\n                          # at the 95% confidence level\n                          level = .95,\n                          # using the standard error\n                          type = \"se\")\n```\n\nAs you can see, 40 hours per week is not contained in this interval, which aligns with our previous conclusion that this finding is significant at the confidence level $\\alpha = .05$.\n\n## Theoretical methods\n\nThe infer package also provides functionality to use theoretical methods for `\"Chisq\"`, `\"F\"` and `\"t\"` test statistics. \n\nGenerally, to find a null distribution using theory-based methods, use the same code that you would use to find the null distribution using randomization-based methods, but skip the `generate()` step. For example, if we wanted to find a null distribution for the relationship between age (`age`) and party identification (`partyid`) using randomization, we could write:\n\n```{r}\n#| message: false\n#| warning: false\nnull_f_distn <- gss %>%\n   specify(age ~ partyid) %>%\n   hypothesize(null = \"independence\") %>%\n   generate(reps = 5000, type = \"permute\") %>%\n   calculate(stat = \"F\")\n```\n\nTo find the null distribution using theory-based methods, instead, skip the `generate()` step entirely:\n\n```{r}\n#| message: false\n#| warning: false\nnull_f_distn_theoretical <- gss %>%\n   specify(age ~ partyid) %>%\n   hypothesize(null = \"independence\") %>%\n   calculate(stat = \"F\")\n```\n\nWe'll calculate the observed statistic to make use of in the following visualizations; this procedure is the same, regardless of the methods used to find the null distribution.\n\n```{r}\n#| message: false\n#| warning: false\nF_hat <- gss %>% \n  specify(age ~ partyid) %>%\n  calculate(stat = \"F\")\n```\n\nNow, instead of just piping the null distribution into `visualize()`, as we would do if we wanted to visualize the randomization-based null distribution, we also need to provide `method = \"theoretical\"` to `visualize()`.\n\n```{r}\n#| message: false\n#| warning: false\nvisualize(null_f_distn_theoretical, method = \"theoretical\") +\n  shade_p_value(obs_stat = F_hat, direction = \"greater\")\n```\n\nTo get a sense of how the theory-based and randomization-based null distributions relate, we can pipe the randomization-based null distribution into `visualize()` and also specify `method = \"both\"`\n\n```{r}\n#| message: false\n#| warning: false\nvisualize(null_f_distn, method = \"both\") +\n  shade_p_value(obs_stat = F_hat, direction = \"greater\")\n```\n\nThat's it! This vignette covers most all of the key functionality of infer. See `help(package = \"infer\")` for a full list of functions and vignettes.\n\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"include-after-body":["../../../resources.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","notebook-preview-download":"Download Notebook","notebook-preview-back":"Back to Article"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.92","theme":["cosmo","../../../styles.scss","../../../styles-frontpage.scss"],"quarto-required":">= 1.3.353","linestretch":1.6,"grid":{"body-width":"840px"},"title":"Hypothesis testing using resampling and tidy data","categories":["statistical analysis","hypothesis testing","bootstraping"],"type":"learn-subsection","weight":4,"description":"Perform common hypothesis tests for statistical inference using flexible functions.\n"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}