{"title":"Custom performance metrics","markdown":{"yaml":{"title":"Custom performance metrics","categories":["developer tools"],"type":"learn-subsection","weight":3,"description":"Create a new performance metric and integrate it with yardstick functions.\n","toc":true,"toc-depth":2,"include-after-body":"../../../resources.html"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n\n```{r}\n#| label: \"load\"\n#| include: false\n#| message: false\n#| warning: false\nlibrary(tidymodels)\nlibrary(rlang)\n\npkgs <- c(\"tidymodels\", \"rlang\")\n```\n\n\n`r article_req_pkgs(pkgs)`\n\nThe [yardstick](https://yardstick.tidymodels.org/) package already includes a large number of metrics, but there's obviously a chance that you might have a custom metric that hasn't been implemented yet. In that case, you can use a few of the tools yardstick exposes to create custom metrics.\n\nWhy create custom metrics? With the infrastructure yardstick provides, you get:\n\n-   Standardization between your metric and other preexisting metrics\n-   Automatic error handling for types and lengths\n-   Automatic selection of binary / multiclass metric implementations\n-   Automatic `NA` handling\n-   Support for grouped data frames\n-   Support for use alongside other metrics in `metric_set()`\n\nThe implementation for metrics differ slightly depending on whether you are implementing a numeric, class, or class probability metric. Examples for numeric and classification metrics are given below. We would encourage you to look into the implementation of `roc_auc()` after reading this vignette if you want to work on a class probability metric.\n\n## Numeric example: MSE\n\nMean squared error (sometimes MSE or from here on, `mse()`) is a numeric metric that measures the average of the squared errors. Numeric metrics are generally the simplest to create with yardstick, as they do not have multiclass implementations. The formula for `mse()` is:\n\n$$ MSE = \\frac{1}{N} \\sum_{i=1}^{N} (truth_i - estimate_i) ^ 2 = mean( (truth - estimate) ^ 2) $$\n\nAll metrics should have a data frame version, and a vector version. The data frame version here will be named `mse()`, and the vector version will be `mse_vec()`.\n\n### Vector implementation\n\nTo start, create the vector version. Generally, all metrics have the same arguments unless the metric requires an extra parameter (such as `beta` in `f_meas()`). To create the vector function, you need to do two things:\n\n1)  Create an internal implementation function, `mse_impl()`.\n2)  Pass on that implementation function to `metric_vec_template()`.\n\nBelow, `mse_impl()` contains the actual implementation of the metric, and takes `truth` and `estimate` as arguments along with any metric specific arguments.\n\nThe yardstick function `metric_vec_template()` accepts the implementation function along with the other arguments to `mse_vec()` and actually executes `mse_impl()`. Additionally, it has a `cls` argument to specify the allowed class type of `truth` and `estimate`. If the classes are the same, a single character class can be passed, and if they are different a character vector of length 2 can be supplied.\n\nThe `metric_vec_template()` helper handles the removal of `NA` values in your metric, so your implementation function does not have to worry about them. It performs type checking using `cls` and also checks that the `estimator` is valid, the second of which is covered in the classification example. This way, all you have to worry about is the core implementation.\n\n```{r}\nlibrary(tidymodels)\n\nmse_vec <- function(truth, estimate, na_rm = TRUE, ...) {\n  \n  mse_impl <- function(truth, estimate) {\n    mean((truth - estimate) ^ 2)\n  }\n  \n  metric_vec_template(\n    metric_impl = mse_impl,\n    truth = truth, \n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"numeric\",\n    ...\n  )\n  \n}\n```\n\nAt this point, you've created the vector version of the mean squared error metric.\n\n```{r}\ndata(\"solubility_test\")\n\nmse_vec(\n  truth = solubility_test$solubility, \n  estimate = solubility_test$prediction\n)\n```\n\nIntelligent error handling is immediately available.\n\n```{r}\n#| error: true\nmse_vec(truth = \"apple\", estimate = 1)\n\nmse_vec(truth = 1, estimate = factor(\"xyz\"))\n```\n\n`NA` values are removed if `na_rm = TRUE` (the default). If `na_rm = FALSE` and any `NA` values are detected, then the metric automatically returns `NA`.\n\n```{r}\n# NA values removed\nmse_vec(truth = c(NA, .5, .4), estimate = c(1, .6, .5))\n\n# NA returned\nmse_vec(truth = c(NA, .5, .4), estimate = c(1, .6, .5), na_rm = FALSE)\n```\n\n### Data frame implementation\n\nThe data frame version of the metric should be fairly simple. It is a generic function with a `data.frame` method that calls the yardstick helper, `metric_summarizer()`, and passes along the `mse_vec()` function to it along with versions of `truth` and `estimate` that have been wrapped in `rlang::enquo()` and then unquoted with `!!` so that non-standard evaluation can be supported.\n\n```{r}\nlibrary(rlang)\n\nmse <- function(data, ...) {\n  UseMethod(\"mse\")\n}\n\nmse <- new_numeric_metric(mse, direction = \"minimize\")\n\nmse.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {\n  \n  metric_summarizer(\n    metric_nm = \"mse\",\n    metric_fn = mse_vec,\n    data = data,\n    truth = !! enquo(truth),\n    estimate = !! enquo(estimate), \n    na_rm = na_rm,\n    ...\n  )\n  \n}\n```\n\nAnd that's it. The yardstick package handles the rest with an internal call to `summarise()`.\n\n```{r}\n#| error: false\n#| eval: false\nmse(solubility_test, truth = solubility, estimate = prediction)\n\n# Error handling\nmse(solubility_test, truth = solubility, estimate = factor(\"xyz\"))\n```\n\nLet's test it out on a grouped data frame.\n\n```{r}\n#| message: false\nlibrary(dplyr)\n\nset.seed(1234)\nsize <- 100\ntimes <- 10\n\n# create 10 resamples\nsolubility_resampled <- bind_rows(\n  replicate(\n    n = times,\n    expr = sample_n(solubility_test, size, replace = TRUE),\n    simplify = FALSE\n  ),\n  .id = \"resample\"\n)\n\nsolubility_resampled %>%\n  group_by(resample) %>%\n  mse(solubility, prediction)\n```\n\n## Class example: miss rate\n\nMiss rate is another name for the false negative rate, and is a classification metric in the same family as `sens()` and `spec()`. It follows the formula:\n\n$$ miss\\_rate = \\frac{FN}{FN + TP} $$\n\nThis metric, like other classification metrics, is more easily computed when expressed as a confusion matrix. As you will see in the example, you can achieve this with a call to `base::table(estimate, truth)` which correctly puts the \"correct\" result in the columns of the confusion matrix.\n\nClassification metrics are more complicated than numeric ones because you have to think about extensions to the multiclass case. For now, let's start with the binary case.\n\n### Vector implementation\n\nThe vector implementation for classification metrics initially has the same setup as numeric metrics, but has an additional argument, `estimator` that determines the type of estimator to use (binary or some kind of multiclass implementation or averaging). This argument is auto-selected for the user, so default it to `NULL`. Additionally, pass it along to `metric_vec_template()` so that it can check the provided `estimator` against the classes of `truth` and `estimate` to see if they are allowed.\n\n```{r}\n# Logic for `event_level`\nevent_col <- function(xtab, event_level) {\n  if (identical(event_level, \"first\")) {\n    colnames(xtab)[[1]]\n  } else {\n    colnames(xtab)[[2]]\n  }\n}\n\nmiss_rate_vec <- function(truth, \n                          estimate, \n                          estimator = NULL, \n                          na_rm = TRUE, \n                          event_level = \"first\",\n                          ...) {\n  estimator <- finalize_estimator(truth, estimator)\n  \n  miss_rate_impl <- function(truth, estimate) {\n    # Create \n    xtab <- table(estimate, truth)\n    col <- event_col(xtab, event_level)\n    col2 <- setdiff(colnames(xtab), col)\n    \n    tp <- xtab[col, col]\n    fn <- xtab[col2, col]\n    \n    fn / (fn + tp)\n  }\n  \n  metric_vec_template(\n    metric_impl = miss_rate_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"factor\",\n    estimator = estimator,\n    ...\n  )\n}\n```\n\nAnother change from the numeric metric is that a call to `finalize_estimator()` is made. This is the infrastructure that auto-selects the type of estimator to use.\n\n```{r}\ndata(\"two_class_example\")\nmiss_rate_vec(two_class_example$truth, two_class_example$predicted)\n```\n\nWhat happens if you try and pass in a multiclass result?\n\n```{r}\ndata(\"hpc_cv\")\nfold1 <- filter(hpc_cv, Resample == \"Fold01\")\nmiss_rate_vec(fold1$obs, fold1$pred)\n```\n\nThis isn't great, as currently multiclass `miss_rate()` isn't supported and it would have been better to throw an error if the `estimator` was not `\"binary\"`. Currently, `finalize_estimator()` uses its default implementation which selected `\"macro\"` as the `estimator` since `truth` was a factor with more than 2 classes. When we implement multiclass averaging, this is what you want, but if your metric only works with a binary implementation (or has other specialized multiclass versions), you might want to guard against this.\n\nTo fix this, a generic counterpart to `finalize_estimator()`, called `finalize_estimator_internal()`, exists that helps you restrict the input types. If you provide a method to `finalize_estimator_internal()` where the method name is the same as your metric name, and then set the `metric_class` argument in `finalize_estimator()` to be the same thing, you can control how the auto-selection of the `estimator` is handled.\n\nDon't worry about the `metric_dispatcher` argument. This is handled for you and just exists as a dummy argument to dispatch off of.\n\nIt is also good practice to call `validate_estimator()` which handles the case where a user passed in the estimator themselves. This validates that the supplied `estimator` is one of the allowed types and error otherwise.\n\n```{r}\n#| error: false\nfinalize_estimator_internal.miss_rate <- function(metric_dispatcher, x, estimator) {\n  \n  validate_estimator(estimator, estimator_override = \"binary\")\n  if (!is.null(estimator)) {\n    return(estimator)\n  }\n  \n  lvls <- levels(x)\n  if (length(lvls) > 2) {\n    stop(\"A multiclass `truth` input was provided, but only `binary` is supported.\")\n  } \n  \"binary\"\n}\n\nmiss_rate_vec <- function(truth, \n                          estimate, \n                          estimator = NULL, \n                          na_rm = TRUE, \n                          event_level = \"first\",\n                          ...) {\n  # calls finalize_estimator_internal() internally\n  estimator <- finalize_estimator(truth, estimator, metric_class = \"miss_rate\")\n  \n  miss_rate_impl <- function(truth, estimate) {\n    # Create \n    xtab <- table(estimate, truth)\n    col <- event_col(xtab, event_level)\n    col2 <- setdiff(colnames(xtab), col)\n    \n    tp <- xtab[col, col]\n    fn <- xtab[col2, col]\n    \n    fn / (fn + tp)\n    \n  }\n  \n  metric_vec_template(\n    metric_impl = miss_rate_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"factor\",\n    estimator = estimator,\n    ...\n  )\n}\n\n# Error thrown by our custom handler\n# miss_rate_vec(fold1$obs, fold1$pred)\n\n# Error thrown by validate_estimator()\n# miss_rate_vec(fold1$obs, fold1$pred, estimator = \"macro\")\n```\n\n### Supporting multiclass miss rate\n\nLike many other classification metrics such as `precision()` or `recall()`, miss rate does not have a natural multiclass extension, but one can be created using methods such as macro, weighted macro, and micro averaging. If you have not, I encourage you to read `vignette(\"multiclass\", \"yardstick\")` for more information about how these methods work.\n\nGenerally, they require more effort to get right than the binary case, especially if you want to have a performant version. Luckily, a somewhat standard template is used in yardstick and can be used here as well.\n\nLet's first remove the \"binary\" restriction we created earlier.\n\n```{r}\nrm(finalize_estimator_internal.miss_rate)\n```\n\nThe main changes below are:\n\n-   The binary implementation is moved to `miss_rate_binary()`.\n\n-   `miss_rate_estimator_impl()` is a helper function for switching between binary and multiclass implementations. It also applies the weighting required for multiclass estimators. It is called from `miss_rate_impl()` and also accepts the `estimator` argument using R's function scoping rules.\n\n-   `miss_rate_multiclass()` provides the implementation for the multiclass case. It calculates the true positive and false negative values as vectors with one value per class. For the macro case, it returns a vector of miss rate calculations, and for micro, it first sums the individual pieces and returns a single miss rate calculation. In the macro case, the vector is then weighted appropriately in `miss_rate_estimator_impl()` depending on whether or not it was macro or weighted macro.\n\n```{r}\nmiss_rate_vec <- function(truth, \n                          estimate, \n                          estimator = NULL, \n                          na_rm = TRUE, \n                          event_level = \"first\",\n                          ...) {\n  # calls finalize_estimator_internal() internally\n  estimator <- finalize_estimator(truth, estimator, metric_class = \"miss_rate\")\n  \n  miss_rate_impl <- function(truth, estimate) {\n    xtab <- table(estimate, truth)\n    # Rather than implement the actual method here, we rely on\n    # an *_estimator_impl() function that can handle binary\n    # and multiclass cases\n    miss_rate_estimator_impl(xtab, estimator, event_level)\n  }\n  \n  metric_vec_template(\n    metric_impl = miss_rate_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"factor\",\n    estimator = estimator,\n    ...\n  )\n}\n\n\n# This function switches between binary and multiclass implementations\nmiss_rate_estimator_impl <- function(data, estimator, event_level) {\n  if(estimator == \"binary\") {\n    miss_rate_binary(data, event_level)\n  } else {\n    # Encapsulates the macro, macro weighted, and micro cases\n    wt <- get_weights(data, estimator)\n    res <- miss_rate_multiclass(data, estimator)\n    weighted.mean(res, wt)\n  }\n}\n\n\nmiss_rate_binary <- function(data, event_level) {\n  col <- event_col(data, event_level)\n  col2 <- setdiff(colnames(data), col)\n  \n  tp <- data[col, col]\n  fn <- data[col2, col]\n  \n  fn / (fn + tp)\n}\n\nmiss_rate_multiclass <- function(data, estimator) {\n  \n  # We need tp and fn for all classes individually\n  # we can get this by taking advantage of the fact\n  # that tp + fn = colSums(data)\n  tp <- diag(data)\n  tpfn <- colSums(data)\n  fn <- tpfn - tp\n  \n  # If using a micro estimator, we sum the individual\n  # pieces before performing the miss rate calculation\n  if (estimator == \"micro\") {\n    tp <- sum(tp)\n    fn <- sum(fn)\n  }\n  \n  # return the vector \n  tp / (tp + fn)\n}\n```\n\nFor the macro case, this separation of weighting from the core implementation might seem strange, but there is good reason for it. Some metrics are combinations of other metrics, and it is nice to be able to reuse code when calculating more complex metrics. For example, `f_meas()` is a combination of `recall()` and `precision()`. When calculating a macro averaged `f_meas()`, the weighting must be applied 1 time, at the very end of the calculation. `recall_multiclass()` and `precision_multiclass()` are defined similarly to how `miss_rate_multiclass()` is defined and returns the unweighted vector of calculations. This means we can directly use this in `f_meas()`, and then weight everything once at the end of that calculation.\n\nLet's try it out now:\n\n```{r}\n# two class\nmiss_rate_vec(two_class_example$truth, two_class_example$predicted)\n\n# multiclass\nmiss_rate_vec(fold1$obs, fold1$pred)\n```\n\n#### Data frame implementation\n\nLuckily, the data frame implementation is as simple as the numeric case, we just need to add an extra `estimator` argument and pass that through.\n\n```{r}\nmiss_rate <- function(data, ...) {\n  UseMethod(\"miss_rate\")\n}\n\nmiss_rate <- new_class_metric(miss_rate, direction = \"minimize\")\n\nmiss_rate.data.frame <- function(data, \n                                 truth, \n                                 estimate, \n                                 estimator = NULL, \n                                 na_rm = TRUE, \n                                 event_level = \"first\",\n                                 ...) {\n  metric_summarizer(\n    metric_nm = \"miss_rate\",\n    metric_fn = miss_rate_vec,\n    data = data,\n    truth = !! enquo(truth),\n    estimate = !! enquo(estimate), \n    estimator = estimator,\n    na_rm = na_rm,\n    event_level = event_level,\n    ...\n  )\n}\n```\n\n```{r}\n#| error: false\n#| eval: false\n# Macro weighted automatically selected\nfold1 %>%\n  miss_rate(obs, pred)\n\n# Switch to micro\nfold1 %>%\n  miss_rate(obs, pred, estimator = \"micro\")\n\n# Macro weighted by resample\nhpc_cv %>%\n  group_by(Resample) %>%\n  miss_rate(obs, pred, estimator = \"macro_weighted\")\n\n# Error handling\nmiss_rate(hpc_cv, obs, VF)\n```\n\n## Using custom metrics\n\nThe `metric_set()` function validates that all metric functions are of the same metric type by checking the class of the function. If any metrics are not of the right class, `metric_set()` fails. By using `new_numeric_metric()` and `new_class_metric()` in the above custom metrics, they work out of the box without any additional adjustments.\n\n```{r}\nnumeric_mets <- metric_set(mse, rmse)\n\nnumeric_mets(solubility_test, solubility, prediction)\n```\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(\"yardstick\")\n```\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n\n```{r}\n#| label: \"load\"\n#| include: false\n#| message: false\n#| warning: false\nlibrary(tidymodels)\nlibrary(rlang)\n\npkgs <- c(\"tidymodels\", \"rlang\")\n```\n\n## Introduction\n\n`r article_req_pkgs(pkgs)`\n\nThe [yardstick](https://yardstick.tidymodels.org/) package already includes a large number of metrics, but there's obviously a chance that you might have a custom metric that hasn't been implemented yet. In that case, you can use a few of the tools yardstick exposes to create custom metrics.\n\nWhy create custom metrics? With the infrastructure yardstick provides, you get:\n\n-   Standardization between your metric and other preexisting metrics\n-   Automatic error handling for types and lengths\n-   Automatic selection of binary / multiclass metric implementations\n-   Automatic `NA` handling\n-   Support for grouped data frames\n-   Support for use alongside other metrics in `metric_set()`\n\nThe implementation for metrics differ slightly depending on whether you are implementing a numeric, class, or class probability metric. Examples for numeric and classification metrics are given below. We would encourage you to look into the implementation of `roc_auc()` after reading this vignette if you want to work on a class probability metric.\n\n## Numeric example: MSE\n\nMean squared error (sometimes MSE or from here on, `mse()`) is a numeric metric that measures the average of the squared errors. Numeric metrics are generally the simplest to create with yardstick, as they do not have multiclass implementations. The formula for `mse()` is:\n\n$$ MSE = \\frac{1}{N} \\sum_{i=1}^{N} (truth_i - estimate_i) ^ 2 = mean( (truth - estimate) ^ 2) $$\n\nAll metrics should have a data frame version, and a vector version. The data frame version here will be named `mse()`, and the vector version will be `mse_vec()`.\n\n### Vector implementation\n\nTo start, create the vector version. Generally, all metrics have the same arguments unless the metric requires an extra parameter (such as `beta` in `f_meas()`). To create the vector function, you need to do two things:\n\n1)  Create an internal implementation function, `mse_impl()`.\n2)  Pass on that implementation function to `metric_vec_template()`.\n\nBelow, `mse_impl()` contains the actual implementation of the metric, and takes `truth` and `estimate` as arguments along with any metric specific arguments.\n\nThe yardstick function `metric_vec_template()` accepts the implementation function along with the other arguments to `mse_vec()` and actually executes `mse_impl()`. Additionally, it has a `cls` argument to specify the allowed class type of `truth` and `estimate`. If the classes are the same, a single character class can be passed, and if they are different a character vector of length 2 can be supplied.\n\nThe `metric_vec_template()` helper handles the removal of `NA` values in your metric, so your implementation function does not have to worry about them. It performs type checking using `cls` and also checks that the `estimator` is valid, the second of which is covered in the classification example. This way, all you have to worry about is the core implementation.\n\n```{r}\nlibrary(tidymodels)\n\nmse_vec <- function(truth, estimate, na_rm = TRUE, ...) {\n  \n  mse_impl <- function(truth, estimate) {\n    mean((truth - estimate) ^ 2)\n  }\n  \n  metric_vec_template(\n    metric_impl = mse_impl,\n    truth = truth, \n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"numeric\",\n    ...\n  )\n  \n}\n```\n\nAt this point, you've created the vector version of the mean squared error metric.\n\n```{r}\ndata(\"solubility_test\")\n\nmse_vec(\n  truth = solubility_test$solubility, \n  estimate = solubility_test$prediction\n)\n```\n\nIntelligent error handling is immediately available.\n\n```{r}\n#| error: true\nmse_vec(truth = \"apple\", estimate = 1)\n\nmse_vec(truth = 1, estimate = factor(\"xyz\"))\n```\n\n`NA` values are removed if `na_rm = TRUE` (the default). If `na_rm = FALSE` and any `NA` values are detected, then the metric automatically returns `NA`.\n\n```{r}\n# NA values removed\nmse_vec(truth = c(NA, .5, .4), estimate = c(1, .6, .5))\n\n# NA returned\nmse_vec(truth = c(NA, .5, .4), estimate = c(1, .6, .5), na_rm = FALSE)\n```\n\n### Data frame implementation\n\nThe data frame version of the metric should be fairly simple. It is a generic function with a `data.frame` method that calls the yardstick helper, `metric_summarizer()`, and passes along the `mse_vec()` function to it along with versions of `truth` and `estimate` that have been wrapped in `rlang::enquo()` and then unquoted with `!!` so that non-standard evaluation can be supported.\n\n```{r}\nlibrary(rlang)\n\nmse <- function(data, ...) {\n  UseMethod(\"mse\")\n}\n\nmse <- new_numeric_metric(mse, direction = \"minimize\")\n\nmse.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {\n  \n  metric_summarizer(\n    metric_nm = \"mse\",\n    metric_fn = mse_vec,\n    data = data,\n    truth = !! enquo(truth),\n    estimate = !! enquo(estimate), \n    na_rm = na_rm,\n    ...\n  )\n  \n}\n```\n\nAnd that's it. The yardstick package handles the rest with an internal call to `summarise()`.\n\n```{r}\n#| error: false\n#| eval: false\nmse(solubility_test, truth = solubility, estimate = prediction)\n\n# Error handling\nmse(solubility_test, truth = solubility, estimate = factor(\"xyz\"))\n```\n\nLet's test it out on a grouped data frame.\n\n```{r}\n#| message: false\nlibrary(dplyr)\n\nset.seed(1234)\nsize <- 100\ntimes <- 10\n\n# create 10 resamples\nsolubility_resampled <- bind_rows(\n  replicate(\n    n = times,\n    expr = sample_n(solubility_test, size, replace = TRUE),\n    simplify = FALSE\n  ),\n  .id = \"resample\"\n)\n\nsolubility_resampled %>%\n  group_by(resample) %>%\n  mse(solubility, prediction)\n```\n\n## Class example: miss rate\n\nMiss rate is another name for the false negative rate, and is a classification metric in the same family as `sens()` and `spec()`. It follows the formula:\n\n$$ miss\\_rate = \\frac{FN}{FN + TP} $$\n\nThis metric, like other classification metrics, is more easily computed when expressed as a confusion matrix. As you will see in the example, you can achieve this with a call to `base::table(estimate, truth)` which correctly puts the \"correct\" result in the columns of the confusion matrix.\n\nClassification metrics are more complicated than numeric ones because you have to think about extensions to the multiclass case. For now, let's start with the binary case.\n\n### Vector implementation\n\nThe vector implementation for classification metrics initially has the same setup as numeric metrics, but has an additional argument, `estimator` that determines the type of estimator to use (binary or some kind of multiclass implementation or averaging). This argument is auto-selected for the user, so default it to `NULL`. Additionally, pass it along to `metric_vec_template()` so that it can check the provided `estimator` against the classes of `truth` and `estimate` to see if they are allowed.\n\n```{r}\n# Logic for `event_level`\nevent_col <- function(xtab, event_level) {\n  if (identical(event_level, \"first\")) {\n    colnames(xtab)[[1]]\n  } else {\n    colnames(xtab)[[2]]\n  }\n}\n\nmiss_rate_vec <- function(truth, \n                          estimate, \n                          estimator = NULL, \n                          na_rm = TRUE, \n                          event_level = \"first\",\n                          ...) {\n  estimator <- finalize_estimator(truth, estimator)\n  \n  miss_rate_impl <- function(truth, estimate) {\n    # Create \n    xtab <- table(estimate, truth)\n    col <- event_col(xtab, event_level)\n    col2 <- setdiff(colnames(xtab), col)\n    \n    tp <- xtab[col, col]\n    fn <- xtab[col2, col]\n    \n    fn / (fn + tp)\n  }\n  \n  metric_vec_template(\n    metric_impl = miss_rate_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"factor\",\n    estimator = estimator,\n    ...\n  )\n}\n```\n\nAnother change from the numeric metric is that a call to `finalize_estimator()` is made. This is the infrastructure that auto-selects the type of estimator to use.\n\n```{r}\ndata(\"two_class_example\")\nmiss_rate_vec(two_class_example$truth, two_class_example$predicted)\n```\n\nWhat happens if you try and pass in a multiclass result?\n\n```{r}\ndata(\"hpc_cv\")\nfold1 <- filter(hpc_cv, Resample == \"Fold01\")\nmiss_rate_vec(fold1$obs, fold1$pred)\n```\n\nThis isn't great, as currently multiclass `miss_rate()` isn't supported and it would have been better to throw an error if the `estimator` was not `\"binary\"`. Currently, `finalize_estimator()` uses its default implementation which selected `\"macro\"` as the `estimator` since `truth` was a factor with more than 2 classes. When we implement multiclass averaging, this is what you want, but if your metric only works with a binary implementation (or has other specialized multiclass versions), you might want to guard against this.\n\nTo fix this, a generic counterpart to `finalize_estimator()`, called `finalize_estimator_internal()`, exists that helps you restrict the input types. If you provide a method to `finalize_estimator_internal()` where the method name is the same as your metric name, and then set the `metric_class` argument in `finalize_estimator()` to be the same thing, you can control how the auto-selection of the `estimator` is handled.\n\nDon't worry about the `metric_dispatcher` argument. This is handled for you and just exists as a dummy argument to dispatch off of.\n\nIt is also good practice to call `validate_estimator()` which handles the case where a user passed in the estimator themselves. This validates that the supplied `estimator` is one of the allowed types and error otherwise.\n\n```{r}\n#| error: false\nfinalize_estimator_internal.miss_rate <- function(metric_dispatcher, x, estimator) {\n  \n  validate_estimator(estimator, estimator_override = \"binary\")\n  if (!is.null(estimator)) {\n    return(estimator)\n  }\n  \n  lvls <- levels(x)\n  if (length(lvls) > 2) {\n    stop(\"A multiclass `truth` input was provided, but only `binary` is supported.\")\n  } \n  \"binary\"\n}\n\nmiss_rate_vec <- function(truth, \n                          estimate, \n                          estimator = NULL, \n                          na_rm = TRUE, \n                          event_level = \"first\",\n                          ...) {\n  # calls finalize_estimator_internal() internally\n  estimator <- finalize_estimator(truth, estimator, metric_class = \"miss_rate\")\n  \n  miss_rate_impl <- function(truth, estimate) {\n    # Create \n    xtab <- table(estimate, truth)\n    col <- event_col(xtab, event_level)\n    col2 <- setdiff(colnames(xtab), col)\n    \n    tp <- xtab[col, col]\n    fn <- xtab[col2, col]\n    \n    fn / (fn + tp)\n    \n  }\n  \n  metric_vec_template(\n    metric_impl = miss_rate_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"factor\",\n    estimator = estimator,\n    ...\n  )\n}\n\n# Error thrown by our custom handler\n# miss_rate_vec(fold1$obs, fold1$pred)\n\n# Error thrown by validate_estimator()\n# miss_rate_vec(fold1$obs, fold1$pred, estimator = \"macro\")\n```\n\n### Supporting multiclass miss rate\n\nLike many other classification metrics such as `precision()` or `recall()`, miss rate does not have a natural multiclass extension, but one can be created using methods such as macro, weighted macro, and micro averaging. If you have not, I encourage you to read `vignette(\"multiclass\", \"yardstick\")` for more information about how these methods work.\n\nGenerally, they require more effort to get right than the binary case, especially if you want to have a performant version. Luckily, a somewhat standard template is used in yardstick and can be used here as well.\n\nLet's first remove the \"binary\" restriction we created earlier.\n\n```{r}\nrm(finalize_estimator_internal.miss_rate)\n```\n\nThe main changes below are:\n\n-   The binary implementation is moved to `miss_rate_binary()`.\n\n-   `miss_rate_estimator_impl()` is a helper function for switching between binary and multiclass implementations. It also applies the weighting required for multiclass estimators. It is called from `miss_rate_impl()` and also accepts the `estimator` argument using R's function scoping rules.\n\n-   `miss_rate_multiclass()` provides the implementation for the multiclass case. It calculates the true positive and false negative values as vectors with one value per class. For the macro case, it returns a vector of miss rate calculations, and for micro, it first sums the individual pieces and returns a single miss rate calculation. In the macro case, the vector is then weighted appropriately in `miss_rate_estimator_impl()` depending on whether or not it was macro or weighted macro.\n\n```{r}\nmiss_rate_vec <- function(truth, \n                          estimate, \n                          estimator = NULL, \n                          na_rm = TRUE, \n                          event_level = \"first\",\n                          ...) {\n  # calls finalize_estimator_internal() internally\n  estimator <- finalize_estimator(truth, estimator, metric_class = \"miss_rate\")\n  \n  miss_rate_impl <- function(truth, estimate) {\n    xtab <- table(estimate, truth)\n    # Rather than implement the actual method here, we rely on\n    # an *_estimator_impl() function that can handle binary\n    # and multiclass cases\n    miss_rate_estimator_impl(xtab, estimator, event_level)\n  }\n  \n  metric_vec_template(\n    metric_impl = miss_rate_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"factor\",\n    estimator = estimator,\n    ...\n  )\n}\n\n\n# This function switches between binary and multiclass implementations\nmiss_rate_estimator_impl <- function(data, estimator, event_level) {\n  if(estimator == \"binary\") {\n    miss_rate_binary(data, event_level)\n  } else {\n    # Encapsulates the macro, macro weighted, and micro cases\n    wt <- get_weights(data, estimator)\n    res <- miss_rate_multiclass(data, estimator)\n    weighted.mean(res, wt)\n  }\n}\n\n\nmiss_rate_binary <- function(data, event_level) {\n  col <- event_col(data, event_level)\n  col2 <- setdiff(colnames(data), col)\n  \n  tp <- data[col, col]\n  fn <- data[col2, col]\n  \n  fn / (fn + tp)\n}\n\nmiss_rate_multiclass <- function(data, estimator) {\n  \n  # We need tp and fn for all classes individually\n  # we can get this by taking advantage of the fact\n  # that tp + fn = colSums(data)\n  tp <- diag(data)\n  tpfn <- colSums(data)\n  fn <- tpfn - tp\n  \n  # If using a micro estimator, we sum the individual\n  # pieces before performing the miss rate calculation\n  if (estimator == \"micro\") {\n    tp <- sum(tp)\n    fn <- sum(fn)\n  }\n  \n  # return the vector \n  tp / (tp + fn)\n}\n```\n\nFor the macro case, this separation of weighting from the core implementation might seem strange, but there is good reason for it. Some metrics are combinations of other metrics, and it is nice to be able to reuse code when calculating more complex metrics. For example, `f_meas()` is a combination of `recall()` and `precision()`. When calculating a macro averaged `f_meas()`, the weighting must be applied 1 time, at the very end of the calculation. `recall_multiclass()` and `precision_multiclass()` are defined similarly to how `miss_rate_multiclass()` is defined and returns the unweighted vector of calculations. This means we can directly use this in `f_meas()`, and then weight everything once at the end of that calculation.\n\nLet's try it out now:\n\n```{r}\n# two class\nmiss_rate_vec(two_class_example$truth, two_class_example$predicted)\n\n# multiclass\nmiss_rate_vec(fold1$obs, fold1$pred)\n```\n\n#### Data frame implementation\n\nLuckily, the data frame implementation is as simple as the numeric case, we just need to add an extra `estimator` argument and pass that through.\n\n```{r}\nmiss_rate <- function(data, ...) {\n  UseMethod(\"miss_rate\")\n}\n\nmiss_rate <- new_class_metric(miss_rate, direction = \"minimize\")\n\nmiss_rate.data.frame <- function(data, \n                                 truth, \n                                 estimate, \n                                 estimator = NULL, \n                                 na_rm = TRUE, \n                                 event_level = \"first\",\n                                 ...) {\n  metric_summarizer(\n    metric_nm = \"miss_rate\",\n    metric_fn = miss_rate_vec,\n    data = data,\n    truth = !! enquo(truth),\n    estimate = !! enquo(estimate), \n    estimator = estimator,\n    na_rm = na_rm,\n    event_level = event_level,\n    ...\n  )\n}\n```\n\n```{r}\n#| error: false\n#| eval: false\n# Macro weighted automatically selected\nfold1 %>%\n  miss_rate(obs, pred)\n\n# Switch to micro\nfold1 %>%\n  miss_rate(obs, pred, estimator = \"micro\")\n\n# Macro weighted by resample\nhpc_cv %>%\n  group_by(Resample) %>%\n  miss_rate(obs, pred, estimator = \"macro_weighted\")\n\n# Error handling\nmiss_rate(hpc_cv, obs, VF)\n```\n\n## Using custom metrics\n\nThe `metric_set()` function validates that all metric functions are of the same metric type by checking the class of the function. If any metrics are not of the right class, `metric_set()` fails. By using `new_numeric_metric()` and `new_class_metric()` in the above custom metrics, they work out of the box without any additional adjustments.\n\n```{r}\nnumeric_mets <- metric_set(mse, rmse)\n\nnumeric_mets(solubility_test, solubility, prediction)\n```\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(\"yardstick\")\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"include-after-body":["../../../resources.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","notebook-preview-download":"Download Notebook","notebook-preview-back":"Back to Article"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.92","theme":["cosmo","../../../styles.scss","../../../styles-frontpage.scss"],"quarto-required":">= 1.3.353","linestretch":1.6,"grid":{"body-width":"840px"},"title":"Custom performance metrics","categories":["developer tools"],"type":"learn-subsection","weight":3,"description":"Create a new performance metric and integrate it with yardstick functions.\n"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}