{"title":"Iterative Bayesian optimization of a classification model","markdown":{"yaml":{"title":"Iterative Bayesian optimization of a classification model","categories":["model tuning","Bayesian optimization","SVMs"],"type":"learn-subsection","weight":3,"description":"Identify the best hyperparameters for a model using Bayesian optimization of iterative search.\n","toc":true,"toc-depth":2,"include-after-body":"../../../resources.html"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n  \n```{r}\n#| label: \"load\"\n#| include: false\nlibrary(tidymodels)\nlibrary(tune)\nlibrary(kernlab)\nlibrary(rlang)\nlibrary(doMC)\nlibrary(themis)\nregisterDoMC(cores = parallel::detectCores())\n\npkgs <- c(\"modeldata\", \"kernlab\", \"tidymodels\", \"themis\")\n\ntheme_set(theme_bw() + theme(legend.position = \"top\"))\n```\n\n\n`r article_req_pkgs(pkgs)`\n\nMany of the examples for model tuning focus on [grid search](/learn/work/tune-svm/). For that method, all the candidate tuning parameter combinations are defined prior to evaluation. Alternatively, _iterative search_ can be used to analyze the existing tuning parameter results and then _predict_ which tuning parameters to try next. \n\nThere are a variety of methods for iterative search and the focus in this article is on _Bayesian optimization_. For more information on this method, these resources might be helpful:\n\n* [_Practical bayesian optimization of machine learning algorithms_](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Practical+Bayesian+Optimization+of+Machine+Learning+Algorithms&btnG=) (2012). J Snoek, H Larochelle, and RP Adams. Advances in neural information.  \n\n* [_A Tutorial on Bayesian Optimization for Machine Learning_](https://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/tutorials/tut8_adams_slides.pdf) (2018). R Adams.\n\n * [_Gaussian Processes for Machine Learning_](http://www.gaussianprocess.org/gpml/) (2006). C E Rasmussen and C Williams.\n\n* [Other articles!](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=\"Bayesian+Optimization\"&btnG=)\n\n\n## Cell segmenting revisited\n\nTo demonstrate this approach to tuning models, let's return to the cell segmentation data from the [Getting Started](/start/resampling/) article on resampling: \n\n```{r}\n#| label: \"import-data\"\nlibrary(tidymodels)\nlibrary(modeldata)\n\n# Load data\ndata(cells)\n\nset.seed(2369)\ntr_te_split <- initial_split(cells %>% select(-case), prop = 3/4)\ncell_train <- training(tr_te_split)\ncell_test  <- testing(tr_te_split)\n\nset.seed(1697)\nfolds <- vfold_cv(cell_train, v = 10)\n```\n\n## The tuning scheme\n\nSince the predictors are highly correlated, we can used a recipe to convert the original predictors to principal component scores. There is also slight class imbalance in these data; about `r floor(mean(cells$class == \"PS\") * 100)`% of the data are poorly segmented. To mitigate this, the data will be down-sampled at the end of the pre-processing so that the number of poorly and well segmented cells occur with equal frequency. We can use a recipe for all this pre-processing, but the number of principal components will need to be _tuned_ so that we have enough (but not too many) representations of the data. \n\n```{r}\n#| label: \"recipe\"\nlibrary(themis)\n\ncell_pre_proc <-\n  recipe(class ~ ., data = cell_train) %>%\n  step_YeoJohnson(all_predictors()) %>%\n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors(), num_comp = tune()) %>%\n  step_downsample(class)\n```\n\nIn this analysis, we will use a support vector machine to model the data. Let's use a radial basis function (RBF) kernel and tune its main parameter ($\\sigma$). Additionally, the main SVM parameter, the cost value, also needs optimization. \n\n```{r}\n#| label: \"model\"\nsvm_mod <-\n  svm_rbf(mode = \"classification\", cost = tune(), rbf_sigma = tune()) %>%\n  set_engine(\"kernlab\")\n```\n\nThese two objects (the recipe and model) will be combined into a single object via the `workflow()` function from the [workflows](https://workflows.tidymodels.org/) package; this object will be used in the optimization process. \n\n```{r}\n#| label: \"workflow\"\nsvm_wflow <-\n  workflow() %>%\n  add_model(svm_mod) %>%\n  add_recipe(cell_pre_proc)\n```\n\nFrom this object, we can derive information about what parameters are slated to be tuned. A parameter set is derived by: \n\n```{r}\n#| label: \"pset\"\nsvm_set <- extract_parameter_set_dials(svm_wflow)\nsvm_set\n```\n\nThe default range for the number of PCA components is rather small for this data set. A member of the parameter set can be modified using the `update()` function. Let's constrain the search to one to twenty components by updating the `num_comp` parameter. Additionally, the lower bound of this parameter is set to zero which specifies that the original predictor set should also be evaluated (i.e., with no PCA step at all): \n\n```{r}\n#| label: \"update\"\nsvm_set <- \n  svm_set %>% \n  update(num_comp = num_comp(c(0L, 20L)))\n```\n\n## Sequential tuning \n\nBayesian optimization is a sequential method that uses a model to predict new candidate parameters for assessment. When scoring potential parameter value, the mean and variance of performance are predicted. The strategy used to define how these two statistical quantities are used is defined by an _acquisition function_. \n\nFor example, one approach for scoring new candidates is to use a confidence bound. Suppose accuracy is being optimized. For a metric that we want to maximize, a lower confidence bound can be used. The multiplier on the standard error (denoted as $\\kappa$) is a value that can be used to make trade-offs between **exploration** and **exploitation**. \n\n * **Exploration** means that the search will consider candidates in untested space.\n\n * **Exploitation** focuses in areas where the previous best results occurred. \n\nThe variance predicted by the Bayesian model is mostly spatial variation; the value will be large for candidate values that are not close to values that have already been evaluated. If the standard error multiplier is high, the search process will be more likely to avoid areas without candidate values in the vicinity. \n\nWe'll use another acquisition function, _expected improvement_, that determines which candidates are likely to be helpful relative to the current best results. This is the default acquisition function. More information on these functions can be found in the [package vignette for acquisition functions](https://tune.tidymodels.org/articles/acquisition_functions.html). \n\n```{r}\n#| label: \"search\"\n#| cache: false\nset.seed(12)\nsearch_res <-\n  svm_wflow %>% \n  tune_bayes(\n    resamples = folds,\n    # To use non-default parameter ranges\n    param_info = svm_set,\n    # Generate five at semi-random to start\n    initial = 5,\n    iter = 50,\n    # How to measure performance?\n    metrics = metric_set(roc_auc),\n    control = control_bayes(no_improve = 30, verbose = TRUE)\n  )\n```\n\nThe resulting tibble is a stacked set of rows of the rsample object with an additional column for the iteration number:\n\n```{r}\n#| label: \"show-iters\"\nsearch_res\n```\n\nAs with grid search, we can summarize the results over resamples:\n\n```{r}\n#| label: \"summarize-iters\"\nestimates <- \n  collect_metrics(search_res) %>% \n  arrange(.iter)\n\nestimates\n```\n\n\nThe best performance of the initial set of candidate values was `AUC = `r max(estimates$mean[estimates$.iter == 0])` `. The best results were achieved at iteration `r estimates$.iter[which.max(estimates$mean)]` with a corresponding AUC value of `r max(estimates$mean)`. The five best results are:\n\n```{r}\n#| label: \"best\"\nshow_best(search_res, metric = \"roc_auc\")\n```\n\nA plot of the search iterations can be created via:\n\n```{r}\n#| label: \"bo-plot\"\nautoplot(search_res, type = \"performance\")\n```\n\nThere are many parameter combinations have roughly equivalent results. \n\nHow did the parameters change over iterations? \n\n\n```{r}\n#| label: \"bo-param-plot\"\n#| fig-width:  9\nautoplot(search_res, type = \"parameters\") + \n  labs(x = \"Iterations\", y = NULL)\n```\n\n\n\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n \n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n  \n```{r}\n#| label: \"load\"\n#| include: false\nlibrary(tidymodels)\nlibrary(tune)\nlibrary(kernlab)\nlibrary(rlang)\nlibrary(doMC)\nlibrary(themis)\nregisterDoMC(cores = parallel::detectCores())\n\npkgs <- c(\"modeldata\", \"kernlab\", \"tidymodels\", \"themis\")\n\ntheme_set(theme_bw() + theme(legend.position = \"top\"))\n```\n\n## Introduction\n\n`r article_req_pkgs(pkgs)`\n\nMany of the examples for model tuning focus on [grid search](/learn/work/tune-svm/). For that method, all the candidate tuning parameter combinations are defined prior to evaluation. Alternatively, _iterative search_ can be used to analyze the existing tuning parameter results and then _predict_ which tuning parameters to try next. \n\nThere are a variety of methods for iterative search and the focus in this article is on _Bayesian optimization_. For more information on this method, these resources might be helpful:\n\n* [_Practical bayesian optimization of machine learning algorithms_](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Practical+Bayesian+Optimization+of+Machine+Learning+Algorithms&btnG=) (2012). J Snoek, H Larochelle, and RP Adams. Advances in neural information.  \n\n* [_A Tutorial on Bayesian Optimization for Machine Learning_](https://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/tutorials/tut8_adams_slides.pdf) (2018). R Adams.\n\n * [_Gaussian Processes for Machine Learning_](http://www.gaussianprocess.org/gpml/) (2006). C E Rasmussen and C Williams.\n\n* [Other articles!](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=\"Bayesian+Optimization\"&btnG=)\n\n\n## Cell segmenting revisited\n\nTo demonstrate this approach to tuning models, let's return to the cell segmentation data from the [Getting Started](/start/resampling/) article on resampling: \n\n```{r}\n#| label: \"import-data\"\nlibrary(tidymodels)\nlibrary(modeldata)\n\n# Load data\ndata(cells)\n\nset.seed(2369)\ntr_te_split <- initial_split(cells %>% select(-case), prop = 3/4)\ncell_train <- training(tr_te_split)\ncell_test  <- testing(tr_te_split)\n\nset.seed(1697)\nfolds <- vfold_cv(cell_train, v = 10)\n```\n\n## The tuning scheme\n\nSince the predictors are highly correlated, we can used a recipe to convert the original predictors to principal component scores. There is also slight class imbalance in these data; about `r floor(mean(cells$class == \"PS\") * 100)`% of the data are poorly segmented. To mitigate this, the data will be down-sampled at the end of the pre-processing so that the number of poorly and well segmented cells occur with equal frequency. We can use a recipe for all this pre-processing, but the number of principal components will need to be _tuned_ so that we have enough (but not too many) representations of the data. \n\n```{r}\n#| label: \"recipe\"\nlibrary(themis)\n\ncell_pre_proc <-\n  recipe(class ~ ., data = cell_train) %>%\n  step_YeoJohnson(all_predictors()) %>%\n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors(), num_comp = tune()) %>%\n  step_downsample(class)\n```\n\nIn this analysis, we will use a support vector machine to model the data. Let's use a radial basis function (RBF) kernel and tune its main parameter ($\\sigma$). Additionally, the main SVM parameter, the cost value, also needs optimization. \n\n```{r}\n#| label: \"model\"\nsvm_mod <-\n  svm_rbf(mode = \"classification\", cost = tune(), rbf_sigma = tune()) %>%\n  set_engine(\"kernlab\")\n```\n\nThese two objects (the recipe and model) will be combined into a single object via the `workflow()` function from the [workflows](https://workflows.tidymodels.org/) package; this object will be used in the optimization process. \n\n```{r}\n#| label: \"workflow\"\nsvm_wflow <-\n  workflow() %>%\n  add_model(svm_mod) %>%\n  add_recipe(cell_pre_proc)\n```\n\nFrom this object, we can derive information about what parameters are slated to be tuned. A parameter set is derived by: \n\n```{r}\n#| label: \"pset\"\nsvm_set <- extract_parameter_set_dials(svm_wflow)\nsvm_set\n```\n\nThe default range for the number of PCA components is rather small for this data set. A member of the parameter set can be modified using the `update()` function. Let's constrain the search to one to twenty components by updating the `num_comp` parameter. Additionally, the lower bound of this parameter is set to zero which specifies that the original predictor set should also be evaluated (i.e., with no PCA step at all): \n\n```{r}\n#| label: \"update\"\nsvm_set <- \n  svm_set %>% \n  update(num_comp = num_comp(c(0L, 20L)))\n```\n\n## Sequential tuning \n\nBayesian optimization is a sequential method that uses a model to predict new candidate parameters for assessment. When scoring potential parameter value, the mean and variance of performance are predicted. The strategy used to define how these two statistical quantities are used is defined by an _acquisition function_. \n\nFor example, one approach for scoring new candidates is to use a confidence bound. Suppose accuracy is being optimized. For a metric that we want to maximize, a lower confidence bound can be used. The multiplier on the standard error (denoted as $\\kappa$) is a value that can be used to make trade-offs between **exploration** and **exploitation**. \n\n * **Exploration** means that the search will consider candidates in untested space.\n\n * **Exploitation** focuses in areas where the previous best results occurred. \n\nThe variance predicted by the Bayesian model is mostly spatial variation; the value will be large for candidate values that are not close to values that have already been evaluated. If the standard error multiplier is high, the search process will be more likely to avoid areas without candidate values in the vicinity. \n\nWe'll use another acquisition function, _expected improvement_, that determines which candidates are likely to be helpful relative to the current best results. This is the default acquisition function. More information on these functions can be found in the [package vignette for acquisition functions](https://tune.tidymodels.org/articles/acquisition_functions.html). \n\n```{r}\n#| label: \"search\"\n#| cache: false\nset.seed(12)\nsearch_res <-\n  svm_wflow %>% \n  tune_bayes(\n    resamples = folds,\n    # To use non-default parameter ranges\n    param_info = svm_set,\n    # Generate five at semi-random to start\n    initial = 5,\n    iter = 50,\n    # How to measure performance?\n    metrics = metric_set(roc_auc),\n    control = control_bayes(no_improve = 30, verbose = TRUE)\n  )\n```\n\nThe resulting tibble is a stacked set of rows of the rsample object with an additional column for the iteration number:\n\n```{r}\n#| label: \"show-iters\"\nsearch_res\n```\n\nAs with grid search, we can summarize the results over resamples:\n\n```{r}\n#| label: \"summarize-iters\"\nestimates <- \n  collect_metrics(search_res) %>% \n  arrange(.iter)\n\nestimates\n```\n\n\nThe best performance of the initial set of candidate values was `AUC = `r max(estimates$mean[estimates$.iter == 0])` `. The best results were achieved at iteration `r estimates$.iter[which.max(estimates$mean)]` with a corresponding AUC value of `r max(estimates$mean)`. The five best results are:\n\n```{r}\n#| label: \"best\"\nshow_best(search_res, metric = \"roc_auc\")\n```\n\nA plot of the search iterations can be created via:\n\n```{r}\n#| label: \"bo-plot\"\nautoplot(search_res, type = \"performance\")\n```\n\nThere are many parameter combinations have roughly equivalent results. \n\nHow did the parameters change over iterations? \n\n\n```{r}\n#| label: \"bo-param-plot\"\n#| fig-width:  9\nautoplot(search_res, type = \"parameters\") + \n  labs(x = \"Iterations\", y = NULL)\n```\n\n\n\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"include-after-body":["../../../resources.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","notebook-preview-download":"Download Notebook","notebook-preview-back":"Back to Article"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.92","theme":["cosmo","../../../styles.scss","../../../styles-frontpage.scss"],"quarto-required":">= 1.3.353","linestretch":1.6,"grid":{"body-width":"840px"},"title":"Iterative Bayesian optimization of a classification model","categories":["model tuning","Bayesian optimization","SVMs"],"type":"learn-subsection","weight":3,"description":"Identify the best hyperparameters for a model using Bayesian optimization of iterative search.\n"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}