{"title":"Tune model parameters","markdown":{"yaml":{"title":"Tune model parameters","weight":4,"categories":["tuning","rsample","parsnip","tune","dials","workflows","yardstick"],"description":"Estimate the best values for hyperparameters that cannot be learned directly during model training.\n","toc-location":"body","toc-depth":2,"toc-title":"","css":"../styles.css","include-after-body":"../repo-actions-delete.html"},"headingText":"Introduction","headingAttr":{"id":"intro","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n\n```{r}\n#| label: \"load\"\n#| include: false\n#| message: false\n#| warning: false\nlibrary(tidymodels)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(kableExtra)\nlibrary(vip)\ntheme_set(theme_bw())\ndoParallel::registerDoParallel()\npkgs <- c(\"tidymodels\", \"rpart\", \"rpart.plot\", \"vip\")\n```\n\n\nSome model parameters cannot be learned directly from a data set during model training; these kinds of parameters are called **hyperparameters**. Some examples of hyperparameters include the number of predictors that are sampled at splits in a tree-based model (we call this `mtry` in tidymodels) or the learning rate in a boosted tree model (we call this `learn_rate`). Instead of learning these kinds of hyperparameters during model training, we can *estimate* the best values for these values by training many models on resampled data sets and exploring how well all these models perform. This process is called **tuning**.\n\n`r article_req_pkgs(pkgs)`\n\n```{r}\n#| eval: false\nlibrary(tidymodels)  # for the tune package, along with the rest of tidymodels\n\n# Helper packages\nlibrary(rpart.plot)  # for visualizing a decision tree\nlibrary(vip)         # for variable importance plots\n```\n\n{{< test-drive url=\"https://rstudio.cloud/project/2674862\" >}}\n\n## The cell image data, revisited {#data}\n\nIn our previous [*Evaluate your model with resampling*](/start/resampling/) article, we introduced a data set of images of cells that were labeled by experts as well-segmented (`WS`) or poorly segmented (`PS`). We trained a [random forest model](/start/resampling/#modeling) to predict which images are segmented well vs. poorly, so that a biologist could filter out poorly segmented cell images in their analysis. We used [resampling](/start/resampling/#resampling) to estimate the performance of our model on this data.\n\n```{r}\n#| label: \"cell-import\"\ndata(cells, package = \"modeldata\")\ncells\n```\n\n## Predicting image segmentation, but better {#why-tune}\n\nRandom forest models are a tree-based ensemble method, and typically perform well with [default hyperparameters](https://bradleyboehmke.github.io/HOML/random-forest.html#out-of-the-box-performance). However, the accuracy of some other tree-based models, such as [boosted tree models](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting) or [decision tree models](https://en.wikipedia.org/wiki/Decision_tree), can be sensitive to the values of hyperparameters. In this article, we will train a **decision tree** model. There are several hyperparameters for decision tree models that can be tuned for better performance. Let's explore:\n\n-   the complexity parameter (which we call `cost_complexity` in tidymodels) for the tree, and\n-   the maximum `tree_depth`.\n\nTuning these hyperparameters can improve model performance because decision tree models are prone to [overfitting](https://bookdown.org/max/FES/important-concepts.html#overfitting). This happens because single tree models tend to fit the training data *too well* --- so well, in fact, that they over-learn patterns present in the training data that end up being detrimental when predicting new data.\n\nWe will tune the model hyperparameters to avoid overfitting. Tuning the value of `cost_complexity` helps by [pruning](https://bradleyboehmke.github.io/HOML/DT.html#pruning) back our tree. It adds a cost, or penalty, to error rates of more complex trees; a cost closer to zero decreases the number tree nodes pruned and is more likely to result in an overfit tree. However, a high cost increases the number of tree nodes pruned and can result in the opposite problem---an underfit tree. Tuning `tree_depth`, on the other hand, helps by [stopping](https://bradleyboehmke.github.io/HOML/DT.html#early-stopping) our tree from growing after it reaches a certain depth. We want to tune these hyperparameters to find what those two values should be for our model to do the best job predicting image segmentation.\n\nBefore we start the tuning process, we split our data into training and testing sets, just like when we trained the model with one default set of hyperparameters. As [before](/start/resampling/), we can use `strata = class` if we want our training and testing sets to be created using stratified sampling so that both have the same proportion of both kinds of segmentation.\n\n```{r}\n#| label: \"cell-split\"\nset.seed(123)\ncell_split <- initial_split(cells %>% select(-case), \n                            strata = class)\ncell_train <- training(cell_split)\ncell_test  <- testing(cell_split)\n```\n\nWe use the training data for tuning the model.\n\n## Tuning hyperparameters {#tuning}\n\nLet's start with the parsnip package, using a [`decision_tree()`](https://parsnip.tidymodels.org/reference/decision_tree.html) model with the [rpart](https://cran.r-project.org/web/packages/rpart/index.html) engine. To tune the decision tree hyperparameters `cost_complexity` and `tree_depth`, we create a model specification that identifies which hyperparameters we plan to tune.\n\n```{r}\n#| label: \"tune-spec\"\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"classification\")\n\ntune_spec\n```\n\nThink of `tune()` here as a placeholder. After the tuning process, we will select a single numeric value for each of these hyperparameters. For now, we specify our parsnip model object and identify the hyperparameters we will `tune()`.\n\nWe can't train this specification on a single data set (such as the entire training set) and learn what the hyperparameter values should be, but we *can* train many models using resampled data and see which models turn out best. We can create a regular grid of values to try using some convenience functions for each hyperparameter:\n\n```{r}\n#| label: \"tree-grid\"\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n```\n\nThe function [`grid_regular()`](https://dials.tidymodels.org/reference/grid_regular.html) is from the [dials](https://dials.tidymodels.org/) package. It chooses sensible values to try for each hyperparameter; here, we asked for 5 of each. Since we have two to tune, `grid_regular()` returns 5 $\\times$ 5 = 25 different possible tuning combinations to try in a tidy tibble format.\n\n```{r}\n#| label: \"tree-grid-tibble\"\ntree_grid\n```\n\nHere, you can see all 5 values of `cost_complexity` ranging up to `r max(tree_grid$cost_complexity)`. These values get repeated for each of the 5 values of `tree_depth`:\n\n```{r}\ntree_grid %>% \n  count(tree_depth)\n```\n\nArmed with our grid filled with 25 candidate decision tree models, let's create [cross-validation folds](/start/resampling/) for tuning:\n\n```{r}\n#| label: \"cell-folds\"\n#| dependson: \"cell-split\"\nset.seed(234)\ncell_folds <- vfold_cv(cell_train)\n```\n\nTuning in tidymodels requires a resampled object created with the [rsample](https://rsample.tidymodels.org/) package.\n\n## Model tuning with a grid {#tune-grid}\n\nWe are ready to tune! Let's use [`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html) to fit models at all the different values we chose for each tuned hyperparameter. There are several options for building the object for tuning:\n\n-   Tune a model specification along with a recipe or model, or\n\n-   Tune a [`workflow()`](https://workflows.tidymodels.org/) that bundles together a model specification and a recipe or model preprocessor.\n\nHere we use a `workflow()` with a straightforward formula; if this model required more involved data preprocessing, we could use `add_recipe()` instead of `add_formula()`.\n\n```{r}\n#| label: \"tree-res\"\n#| dependson: c(\"tune-spec\", \"cell-folds\", \"tree-grid\")\n#| message: false\nset.seed(345)\n\ntree_wf <- workflow() %>%\n  add_model(tune_spec) %>%\n  add_formula(class ~ .)\n\ntree_res <- \n  tree_wf %>% \n  tune_grid(\n    resamples = cell_folds,\n    grid = tree_grid\n    )\n\ntree_res\n```\n\nOnce we have our tuning results, we can both explore them through visualization and then select the best result. The function `collect_metrics()` gives us a tidy tibble with all the results. We had 25 candidate models and two metrics, `accuracy` and `roc_auc`, and we get a row for each `.metric` and model.\n\n```{r}\n#| label: \"collect-trees\"\n#| dependson: \"tree-res\"\ntree_res %>% \n  collect_metrics()\n```\n\nWe might get more out of plotting these results:\n\n```{r}\n#| label: \"best-tree\"\n#| dependson: \"tree-res\"\n#| fig-width:  8\n#| fig-height:  7\ntree_res %>%\n  collect_metrics() %>%\n  mutate(tree_depth = factor(tree_depth)) %>%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n```\n\nWe can see that our \"stubbiest\" tree, with a depth of `r min(tree_grid$tree_depth)`, is the worst model according to both metrics and across all candidate values of `cost_complexity`. Our deepest tree, with a depth of `r max(tree_grid$tree_depth)`, did better. However, the best tree seems to be between these values with a tree depth of 4. The [`show_best()`](https://tune.tidymodels.org/reference/show_best.html) function shows us the top 5 candidate models by default:\n\n```{r}\n#| label: \"show-best-tree\"\n#| dependson: \"tree-res\"\ntree_res %>%\n  show_best(\"accuracy\")\n```\n\nWe can also use the [`select_best()`](https://tune.tidymodels.org/reference/show_best.html) function to pull out the single set of hyperparameter values for our best decision tree model:\n\n```{r}\n#| label: \"select-best-tree\"\n#| dependson: \"tree-res\"\nbest_tree <- tree_res %>%\n  select_best(\"accuracy\")\n\nbest_tree\n```\n\nThese are the values for `tree_depth` and `cost_complexity` that maximize accuracy in this data set of cell images.\n\n## Finalizing our model {#final-model}\n\nWe can update (or \"finalize\") our workflow object `tree_wf` with the values from `select_best()`.\n\n```{r}\n#| label: \"final-wf\"\n#| dependson: \"best-tree\"\nfinal_wf <- \n  tree_wf %>% \n  finalize_workflow(best_tree)\n\nfinal_wf\n```\n\nOur tuning is done!\n\n### The last fit\n\nFinally, let's fit this final model to the training data and use our test data to estimate the model performance we expect to see with new data. We can use the function [`last_fit()`](https://tune.tidymodels.org/reference/last_fit.html) with our finalized model; this function *fits* the finalized model on the full training data set and *evaluates* the finalized model on the testing data.\n\n```{r}\n#| label: \"last-fit\"\n#| dependson: c(\"final-wf\", \"cell-split\")\nfinal_fit <- \n  final_wf %>%\n  last_fit(cell_split) \n\nfinal_fit %>%\n  collect_metrics()\n\nfinal_fit %>%\n  collect_predictions() %>% \n  roc_curve(class, .pred_PS) %>% \n  autoplot()\n```\n\nThe performance metrics from the test set indicate that we did not overfit during our tuning procedure.\n\nThe `final_fit` object contains a finalized, fitted workflow that you can use for predicting on new data or further understanding the results. You may want to extract this object, using [one of the `extract_` helper functions](https://tune.tidymodels.org/reference/extract-tune.html).\n\n```{r}\n#| label: \"last-fit-wf\"\n#| dependson: \"last-fit\"\nfinal_tree <- extract_workflow(final_fit)\nfinal_tree\n```\n\nWe can create a visualization of the decision tree using another helper function to extract the underlying engine-specific fit.\n\n```{r}\n#| label: \"rpart-plot\"\n#| dependson: \"last-fit-wf\"\n#| fig-width:  8\n#| fig-height:  5\nfinal_tree %>%\n  extract_fit_engine() %>%\n  rpart.plot(roundint = FALSE)\n```\n\nPerhaps we would also like to understand what variables are important in this final model. We can use the [vip](https://koalaverse.github.io/vip/) package to estimate variable importance [based on the model's structure](https://koalaverse.github.io/vip/reference/vi_model.html#details).\n\n```{r}\n#| label: \"vip\"\n#| dependson: \"final-tree\"\n#| fig-width:  6\n#| fig-height:  5\nlibrary(vip)\n\nfinal_tree %>% \n  extract_fit_parsnip() %>% \n  vip()\n```\n\nThese are the automated image analysis measurements that are the most important in driving segmentation quality predictions.\n\nWe leave it to the reader to explore whether you can tune a different decision tree hyperparameter. You can explore the [reference docs](/find/parsnip/#models), or use the `args()` function to see which parsnip object arguments are available:\n\n```{r}\nargs(decision_tree)\n```\n\nYou could tune the other hyperparameter we didn't use here, `min_n`, which sets the minimum `n` to split at any node. This is another early stopping method for decision trees that can help prevent overfitting. Use this [searchable table](/find/parsnip/#model-args) to find the original argument for `min_n` in the rpart package ([hint](https://stat.ethz.ch/R-manual/R-devel/library/rpart/html/rpart.control.html)). See whether you can tune a different combination of hyperparameters and/or values to improve a tree's ability to predict cell segmentation quality.\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\"\n#| include: false\n#| message: false\n#| warning: false\nsource(here::here(\"common.R\"))\n```\n\n```{r}\n#| label: \"load\"\n#| include: false\n#| message: false\n#| warning: false\nlibrary(tidymodels)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(kableExtra)\nlibrary(vip)\ntheme_set(theme_bw())\ndoParallel::registerDoParallel()\npkgs <- c(\"tidymodels\", \"rpart\", \"rpart.plot\", \"vip\")\n```\n\n## Introduction {#intro}\n\nSome model parameters cannot be learned directly from a data set during model training; these kinds of parameters are called **hyperparameters**. Some examples of hyperparameters include the number of predictors that are sampled at splits in a tree-based model (we call this `mtry` in tidymodels) or the learning rate in a boosted tree model (we call this `learn_rate`). Instead of learning these kinds of hyperparameters during model training, we can *estimate* the best values for these values by training many models on resampled data sets and exploring how well all these models perform. This process is called **tuning**.\n\n`r article_req_pkgs(pkgs)`\n\n```{r}\n#| eval: false\nlibrary(tidymodels)  # for the tune package, along with the rest of tidymodels\n\n# Helper packages\nlibrary(rpart.plot)  # for visualizing a decision tree\nlibrary(vip)         # for variable importance plots\n```\n\n{{< test-drive url=\"https://rstudio.cloud/project/2674862\" >}}\n\n## The cell image data, revisited {#data}\n\nIn our previous [*Evaluate your model with resampling*](/start/resampling/) article, we introduced a data set of images of cells that were labeled by experts as well-segmented (`WS`) or poorly segmented (`PS`). We trained a [random forest model](/start/resampling/#modeling) to predict which images are segmented well vs. poorly, so that a biologist could filter out poorly segmented cell images in their analysis. We used [resampling](/start/resampling/#resampling) to estimate the performance of our model on this data.\n\n```{r}\n#| label: \"cell-import\"\ndata(cells, package = \"modeldata\")\ncells\n```\n\n## Predicting image segmentation, but better {#why-tune}\n\nRandom forest models are a tree-based ensemble method, and typically perform well with [default hyperparameters](https://bradleyboehmke.github.io/HOML/random-forest.html#out-of-the-box-performance). However, the accuracy of some other tree-based models, such as [boosted tree models](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting) or [decision tree models](https://en.wikipedia.org/wiki/Decision_tree), can be sensitive to the values of hyperparameters. In this article, we will train a **decision tree** model. There are several hyperparameters for decision tree models that can be tuned for better performance. Let's explore:\n\n-   the complexity parameter (which we call `cost_complexity` in tidymodels) for the tree, and\n-   the maximum `tree_depth`.\n\nTuning these hyperparameters can improve model performance because decision tree models are prone to [overfitting](https://bookdown.org/max/FES/important-concepts.html#overfitting). This happens because single tree models tend to fit the training data *too well* --- so well, in fact, that they over-learn patterns present in the training data that end up being detrimental when predicting new data.\n\nWe will tune the model hyperparameters to avoid overfitting. Tuning the value of `cost_complexity` helps by [pruning](https://bradleyboehmke.github.io/HOML/DT.html#pruning) back our tree. It adds a cost, or penalty, to error rates of more complex trees; a cost closer to zero decreases the number tree nodes pruned and is more likely to result in an overfit tree. However, a high cost increases the number of tree nodes pruned and can result in the opposite problem---an underfit tree. Tuning `tree_depth`, on the other hand, helps by [stopping](https://bradleyboehmke.github.io/HOML/DT.html#early-stopping) our tree from growing after it reaches a certain depth. We want to tune these hyperparameters to find what those two values should be for our model to do the best job predicting image segmentation.\n\nBefore we start the tuning process, we split our data into training and testing sets, just like when we trained the model with one default set of hyperparameters. As [before](/start/resampling/), we can use `strata = class` if we want our training and testing sets to be created using stratified sampling so that both have the same proportion of both kinds of segmentation.\n\n```{r}\n#| label: \"cell-split\"\nset.seed(123)\ncell_split <- initial_split(cells %>% select(-case), \n                            strata = class)\ncell_train <- training(cell_split)\ncell_test  <- testing(cell_split)\n```\n\nWe use the training data for tuning the model.\n\n## Tuning hyperparameters {#tuning}\n\nLet's start with the parsnip package, using a [`decision_tree()`](https://parsnip.tidymodels.org/reference/decision_tree.html) model with the [rpart](https://cran.r-project.org/web/packages/rpart/index.html) engine. To tune the decision tree hyperparameters `cost_complexity` and `tree_depth`, we create a model specification that identifies which hyperparameters we plan to tune.\n\n```{r}\n#| label: \"tune-spec\"\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"classification\")\n\ntune_spec\n```\n\nThink of `tune()` here as a placeholder. After the tuning process, we will select a single numeric value for each of these hyperparameters. For now, we specify our parsnip model object and identify the hyperparameters we will `tune()`.\n\nWe can't train this specification on a single data set (such as the entire training set) and learn what the hyperparameter values should be, but we *can* train many models using resampled data and see which models turn out best. We can create a regular grid of values to try using some convenience functions for each hyperparameter:\n\n```{r}\n#| label: \"tree-grid\"\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n```\n\nThe function [`grid_regular()`](https://dials.tidymodels.org/reference/grid_regular.html) is from the [dials](https://dials.tidymodels.org/) package. It chooses sensible values to try for each hyperparameter; here, we asked for 5 of each. Since we have two to tune, `grid_regular()` returns 5 $\\times$ 5 = 25 different possible tuning combinations to try in a tidy tibble format.\n\n```{r}\n#| label: \"tree-grid-tibble\"\ntree_grid\n```\n\nHere, you can see all 5 values of `cost_complexity` ranging up to `r max(tree_grid$cost_complexity)`. These values get repeated for each of the 5 values of `tree_depth`:\n\n```{r}\ntree_grid %>% \n  count(tree_depth)\n```\n\nArmed with our grid filled with 25 candidate decision tree models, let's create [cross-validation folds](/start/resampling/) for tuning:\n\n```{r}\n#| label: \"cell-folds\"\n#| dependson: \"cell-split\"\nset.seed(234)\ncell_folds <- vfold_cv(cell_train)\n```\n\nTuning in tidymodels requires a resampled object created with the [rsample](https://rsample.tidymodels.org/) package.\n\n## Model tuning with a grid {#tune-grid}\n\nWe are ready to tune! Let's use [`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html) to fit models at all the different values we chose for each tuned hyperparameter. There are several options for building the object for tuning:\n\n-   Tune a model specification along with a recipe or model, or\n\n-   Tune a [`workflow()`](https://workflows.tidymodels.org/) that bundles together a model specification and a recipe or model preprocessor.\n\nHere we use a `workflow()` with a straightforward formula; if this model required more involved data preprocessing, we could use `add_recipe()` instead of `add_formula()`.\n\n```{r}\n#| label: \"tree-res\"\n#| dependson: c(\"tune-spec\", \"cell-folds\", \"tree-grid\")\n#| message: false\nset.seed(345)\n\ntree_wf <- workflow() %>%\n  add_model(tune_spec) %>%\n  add_formula(class ~ .)\n\ntree_res <- \n  tree_wf %>% \n  tune_grid(\n    resamples = cell_folds,\n    grid = tree_grid\n    )\n\ntree_res\n```\n\nOnce we have our tuning results, we can both explore them through visualization and then select the best result. The function `collect_metrics()` gives us a tidy tibble with all the results. We had 25 candidate models and two metrics, `accuracy` and `roc_auc`, and we get a row for each `.metric` and model.\n\n```{r}\n#| label: \"collect-trees\"\n#| dependson: \"tree-res\"\ntree_res %>% \n  collect_metrics()\n```\n\nWe might get more out of plotting these results:\n\n```{r}\n#| label: \"best-tree\"\n#| dependson: \"tree-res\"\n#| fig-width:  8\n#| fig-height:  7\ntree_res %>%\n  collect_metrics() %>%\n  mutate(tree_depth = factor(tree_depth)) %>%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n```\n\nWe can see that our \"stubbiest\" tree, with a depth of `r min(tree_grid$tree_depth)`, is the worst model according to both metrics and across all candidate values of `cost_complexity`. Our deepest tree, with a depth of `r max(tree_grid$tree_depth)`, did better. However, the best tree seems to be between these values with a tree depth of 4. The [`show_best()`](https://tune.tidymodels.org/reference/show_best.html) function shows us the top 5 candidate models by default:\n\n```{r}\n#| label: \"show-best-tree\"\n#| dependson: \"tree-res\"\ntree_res %>%\n  show_best(\"accuracy\")\n```\n\nWe can also use the [`select_best()`](https://tune.tidymodels.org/reference/show_best.html) function to pull out the single set of hyperparameter values for our best decision tree model:\n\n```{r}\n#| label: \"select-best-tree\"\n#| dependson: \"tree-res\"\nbest_tree <- tree_res %>%\n  select_best(\"accuracy\")\n\nbest_tree\n```\n\nThese are the values for `tree_depth` and `cost_complexity` that maximize accuracy in this data set of cell images.\n\n## Finalizing our model {#final-model}\n\nWe can update (or \"finalize\") our workflow object `tree_wf` with the values from `select_best()`.\n\n```{r}\n#| label: \"final-wf\"\n#| dependson: \"best-tree\"\nfinal_wf <- \n  tree_wf %>% \n  finalize_workflow(best_tree)\n\nfinal_wf\n```\n\nOur tuning is done!\n\n### The last fit\n\nFinally, let's fit this final model to the training data and use our test data to estimate the model performance we expect to see with new data. We can use the function [`last_fit()`](https://tune.tidymodels.org/reference/last_fit.html) with our finalized model; this function *fits* the finalized model on the full training data set and *evaluates* the finalized model on the testing data.\n\n```{r}\n#| label: \"last-fit\"\n#| dependson: c(\"final-wf\", \"cell-split\")\nfinal_fit <- \n  final_wf %>%\n  last_fit(cell_split) \n\nfinal_fit %>%\n  collect_metrics()\n\nfinal_fit %>%\n  collect_predictions() %>% \n  roc_curve(class, .pred_PS) %>% \n  autoplot()\n```\n\nThe performance metrics from the test set indicate that we did not overfit during our tuning procedure.\n\nThe `final_fit` object contains a finalized, fitted workflow that you can use for predicting on new data or further understanding the results. You may want to extract this object, using [one of the `extract_` helper functions](https://tune.tidymodels.org/reference/extract-tune.html).\n\n```{r}\n#| label: \"last-fit-wf\"\n#| dependson: \"last-fit\"\nfinal_tree <- extract_workflow(final_fit)\nfinal_tree\n```\n\nWe can create a visualization of the decision tree using another helper function to extract the underlying engine-specific fit.\n\n```{r}\n#| label: \"rpart-plot\"\n#| dependson: \"last-fit-wf\"\n#| fig-width:  8\n#| fig-height:  5\nfinal_tree %>%\n  extract_fit_engine() %>%\n  rpart.plot(roundint = FALSE)\n```\n\nPerhaps we would also like to understand what variables are important in this final model. We can use the [vip](https://koalaverse.github.io/vip/) package to estimate variable importance [based on the model's structure](https://koalaverse.github.io/vip/reference/vi_model.html#details).\n\n```{r}\n#| label: \"vip\"\n#| dependson: \"final-tree\"\n#| fig-width:  6\n#| fig-height:  5\nlibrary(vip)\n\nfinal_tree %>% \n  extract_fit_parsnip() %>% \n  vip()\n```\n\nThese are the automated image analysis measurements that are the most important in driving segmentation quality predictions.\n\nWe leave it to the reader to explore whether you can tune a different decision tree hyperparameter. You can explore the [reference docs](/find/parsnip/#models), or use the `args()` function to see which parsnip object arguments are available:\n\n```{r}\nargs(decision_tree)\n```\n\nYou could tune the other hyperparameter we didn't use here, `min_n`, which sets the minimum `n` to split at any node. This is another early stopping method for decision trees that can help prevent overfitting. Use this [searchable table](/find/parsnip/#model-args) to find the original argument for `min_n` in the rpart package ([hint](https://stat.ethz.ch/R-manual/R-devel/library/rpart/html/rpart.control.html)). See whether you can tune a different combination of hyperparameters and/or values to improve a tree's ability to predict cell segmentation quality.\n\n## Session information {#session-info}\n\n```{r}\n#| label: \"si\"\n#| echo: false\nsmall_session(pkgs)\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"css":["../styles.css"],"include-after-body":["../repo-actions-delete.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","notebook-preview-download":"Download Notebook","notebook-preview-back":"Back to Article"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.92","theme":["cosmo","../../styles.scss","../../styles-frontpage.scss"],"quarto-required":">= 1.3.353","linestretch":1.6,"grid":{"body-width":"840px"},"title":"Tune model parameters","weight":4,"categories":["tuning","rsample","parsnip","tune","dials","workflows","yardstick"],"description":"Estimate the best values for hyperparameters that cannot be learned directly during model training.\n","toc-location":"body","toc-title":""},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}