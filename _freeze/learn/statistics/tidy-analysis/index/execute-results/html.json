{
  "hash": "9dd5f07df661cda40b8c331c9af79966",
  "result": {
    "markdown": "---\ntitle: \"Correlation and regression fundamentals with tidy data principles\"\ncategories:\n  - statistical analysis\n  - correlation\n  - tidying results\ntype: learn-subsection\nweight: 1\ndescription: | \n  Analyze the results of correlation tests and simple regression models for many data sets at once.\ntoc: true\ntoc-depth: 2\ninclude-after-body: ../../../resources.html\n---\n\n\n\n\n\n\n## Introduction\n\nThis article only requires the tidymodels package.\n\nWhile the tidymodels package [broom](https://broom.tidyverse.org/) is useful for summarizing the result of a single analysis in a consistent format, it is really designed for high-throughput applications, where you must combine results from multiple analyses. These could be subgroups of data, analyses using different models, bootstrap replicates, permutations, and so on. In particular, it plays well with the `nest()/unnest()` functions from [tidyr](https://tidyr.tidyverse.org/) and the `map()` function in [purrr](https://purrr.tidyverse.org/).\n\n## Correlation analysis\n\nLet's demonstrate this with a simple data set, the built-in `Orange`. We start by coercing `Orange` to a `tibble`. This gives a nicer print method that will be especially useful later on when we start working with list-columns.\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-3_b8c28b0b2e42595af6cd6e42e320211e'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n\ndata(Orange)\n\nOrange <- as_tibble(Orange)\nOrange\n#> # A tibble: 35 × 3\n#>    Tree    age circumference\n#>    <ord> <dbl>         <dbl>\n#>  1 1       118            30\n#>  2 1       484            58\n#>  3 1       664            87\n#>  4 1      1004           115\n#>  5 1      1231           120\n#>  6 1      1372           142\n#>  7 1      1582           145\n#>  8 2       118            33\n#>  9 2       484            69\n#> 10 2       664           111\n#> # ℹ 25 more rows\n```\n:::\n\n\nThis contains 35 observations of three variables: `Tree`, `age`, and `circumference`. `Tree` is a factor with five levels describing five trees. As might be expected, age and circumference are correlated:\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-4_d23aebe621890c33deabf5c88ab2e2e7'}\n\n```{.r .cell-code}\ncor(Orange$age, Orange$circumference)\n#> [1] 0.9135189\n\nlibrary(ggplot2)\n\nggplot(Orange, aes(age, circumference, color = Tree)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](figs/unnamed-chunk-4-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\nSuppose you want to test for correlations individually *within* each tree. You can do this with dplyr's `group_by`:\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-5_91dbe7c00d07ed20010e073d66c2fdda'}\n\n```{.r .cell-code}\nOrange %>% \n  group_by(Tree) %>%\n  summarize(correlation = cor(age, circumference))\n#> # A tibble: 5 × 2\n#>   Tree  correlation\n#>   <ord>       <dbl>\n#> 1 3           0.988\n#> 2 1           0.985\n#> 3 5           0.988\n#> 4 2           0.987\n#> 5 4           0.984\n```\n:::\n\n\n(Note that the correlations are much higher than the aggregated one, and also we can now see the correlation is similar across trees).\n\nSuppose that instead of simply estimating a correlation, we want to perform a hypothesis test with `cor.test()`:\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-6_46fa9b020ddb8990c5c81c2814b26dca'}\n\n```{.r .cell-code}\nct <- cor.test(Orange$age, Orange$circumference)\nct\n#> \n#> \tPearson's product-moment correlation\n#> \n#> data:  Orange$age and Orange$circumference\n#> t = 12.9, df = 33, p-value = 1.931e-14\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.8342364 0.9557955\n#> sample estimates:\n#>       cor \n#> 0.9135189\n```\n:::\n\n\nThis test output contains multiple values we may be interested in. Some are vectors of length 1, such as the p-value and the estimate, and some are longer, such as the confidence interval. We can get this into a nicely organized tibble using the `tidy()` function:\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-7_e76d10df195a88e005ab096c3a46fd52'}\n\n```{.r .cell-code}\ntidy(ct)\n#> # A tibble: 1 × 8\n#>   estimate statistic  p.value parameter conf.low conf.high method    alternative\n#>      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>     <chr>      \n#> 1    0.914      12.9 1.93e-14        33    0.834     0.956 Pearson'… two.sided\n```\n:::\n\n\nOften, we want to perform multiple tests or fit multiple models, each on a different part of the data. In this case, we recommend a `nest-map-unnest` workflow. For example, suppose we want to perform correlation tests for each different tree. We start by `nest`ing our data based on the group of interest:\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-8_3a8c5c9b027a3b1290a315655286c386'}\n\n```{.r .cell-code}\nnested <- \n  Orange %>% \n  nest(data = c(age, circumference))\n```\n:::\n\n\nThen we perform a correlation test for each nested tibble using `purrr::map()`:\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-9_99179b6d70604b3edd0d0107b29bea04'}\n\n```{.r .cell-code}\nnested %>% \n  mutate(test = map(data, ~ cor.test(.x$age, .x$circumference)))\n#> # A tibble: 5 × 3\n#>   Tree  data             test   \n#>   <ord> <list>           <list> \n#> 1 1     <tibble [7 × 2]> <htest>\n#> 2 2     <tibble [7 × 2]> <htest>\n#> 3 3     <tibble [7 × 2]> <htest>\n#> 4 4     <tibble [7 × 2]> <htest>\n#> 5 5     <tibble [7 × 2]> <htest>\n```\n:::\n\n\nThis results in a list-column of S3 objects. We want to tidy each of the objects, which we can also do with `map()`.\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-10_1d2edd8b12d02933b931669b101d050e'}\n\n```{.r .cell-code}\nnested %>% \n  mutate(\n    test = map(data, ~ cor.test(.x$age, .x$circumference)), # S3 list-col\n    tidied = map(test, tidy)\n  ) \n#> # A tibble: 5 × 4\n#>   Tree  data             test    tidied          \n#>   <ord> <list>           <list>  <list>          \n#> 1 1     <tibble [7 × 2]> <htest> <tibble [1 × 8]>\n#> 2 2     <tibble [7 × 2]> <htest> <tibble [1 × 8]>\n#> 3 3     <tibble [7 × 2]> <htest> <tibble [1 × 8]>\n#> 4 4     <tibble [7 × 2]> <htest> <tibble [1 × 8]>\n#> 5 5     <tibble [7 × 2]> <htest> <tibble [1 × 8]>\n```\n:::\n\n\nFinally, we want to unnest the tidied data frames so we can see the results in a flat tibble. All together, this looks like:\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-11_48290f0abce3aa8054e3b335ae75bfdf'}\n\n```{.r .cell-code}\nOrange %>% \n  nest(data = c(age, circumference)) %>% \n  mutate(\n    test = map(data, ~ cor.test(.x$age, .x$circumference)), # S3 list-col\n    tidied = map(test, tidy)\n  ) %>% \n  unnest(cols = tidied) %>% \n  select(-data, -test)\n#> # A tibble: 5 × 9\n#>   Tree  estimate statistic   p.value parameter conf.low conf.high method        \n#>   <ord>    <dbl>     <dbl>     <dbl>     <int>    <dbl>     <dbl> <chr>         \n#> 1 1        0.985      13.0 0.0000485         5    0.901     0.998 Pearson's pro…\n#> 2 2        0.987      13.9 0.0000343         5    0.914     0.998 Pearson's pro…\n#> 3 3        0.988      14.4 0.0000290         5    0.919     0.998 Pearson's pro…\n#> 4 4        0.984      12.5 0.0000573         5    0.895     0.998 Pearson's pro…\n#> 5 5        0.988      14.1 0.0000318         5    0.916     0.998 Pearson's pro…\n#> # ℹ 1 more variable: alternative <chr>\n```\n:::\n\n\n## Regression models\n\nThis type of workflow becomes even more useful when applied to regressions. Untidy output for a regression looks like:\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-12_b1826e99afb5e6d9f52a61ffc0f64eb3'}\n\n```{.r .cell-code}\nlm_fit <- lm(age ~ circumference, data = Orange)\nsummary(lm_fit)\n#> \n#> Call:\n#> lm(formula = age ~ circumference, data = Orange)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -317.88 -140.90  -17.20   96.54  471.16 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    16.6036    78.1406   0.212    0.833    \n#> circumference   7.8160     0.6059  12.900 1.93e-14 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 203.1 on 33 degrees of freedom\n#> Multiple R-squared:  0.8345,\tAdjusted R-squared:  0.8295 \n#> F-statistic: 166.4 on 1 and 33 DF,  p-value: 1.931e-14\n```\n:::\n\n\nWhen we tidy these results, we get multiple rows of output for each model:\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-13_3097cfaa120e9f8a9c38220653816d1e'}\n\n```{.r .cell-code}\ntidy(lm_fit)\n#> # A tibble: 2 × 5\n#>   term          estimate std.error statistic  p.value\n#>   <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)      16.6     78.1       0.212 8.33e- 1\n#> 2 circumference     7.82     0.606    12.9   1.93e-14\n```\n:::\n\n\nNow we can handle multiple regressions at once using exactly the same workflow as before:\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-14_8e75b7bb2abc579c067c0c472239046f'}\n\n```{.r .cell-code}\nOrange %>%\n  nest(data = c(-Tree)) %>% \n  mutate(\n    fit = map(data, ~ lm(age ~ circumference, data = .x)),\n    tidied = map(fit, tidy)\n  ) %>% \n  unnest(tidied) %>% \n  select(-data, -fit)\n#> # A tibble: 10 × 6\n#>    Tree  term          estimate std.error statistic   p.value\n#>    <ord> <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n#>  1 1     (Intercept)    -265.      98.6      -2.68  0.0436   \n#>  2 1     circumference    11.9      0.919    13.0   0.0000485\n#>  3 2     (Intercept)    -132.      83.1      -1.59  0.172    \n#>  4 2     circumference     7.80     0.560    13.9   0.0000343\n#>  5 3     (Intercept)    -210.      85.3      -2.46  0.0574   \n#>  6 3     circumference    12.0      0.835    14.4   0.0000290\n#>  7 4     (Intercept)     -76.5     88.3      -0.867 0.426    \n#>  8 4     circumference     7.17     0.572    12.5   0.0000573\n#>  9 5     (Intercept)     -54.5     76.9      -0.709 0.510    \n#> 10 5     circumference     8.79     0.621    14.1   0.0000318\n```\n:::\n\n\nYou can just as easily use multiple predictors in the regressions, as shown here on the `mtcars` dataset. We nest the data into automatic vs. manual cars (the `am` column), then perform the regression within each nested tibble.\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-15_b9dc82496897bd3f80514e02077a3dd8'}\n\n```{.r .cell-code}\ndata(mtcars)\nmtcars <- as_tibble(mtcars)  # to play nicely with list-cols\nmtcars\n#> # A tibble: 32 × 11\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n#> # ℹ 22 more rows\n\nmtcars %>%\n  nest(data = c(-am)) %>% \n  mutate(\n    fit = map(data, ~ lm(wt ~ mpg + qsec + gear, data = .x)),  # S3 list-col\n    tidied = map(fit, tidy)\n  ) %>% \n  unnest(tidied) %>% \n  select(-data, -fit)\n#> # A tibble: 8 × 6\n#>      am term        estimate std.error statistic  p.value\n#>   <dbl> <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1     1 (Intercept)   4.28      3.46      1.24   0.247   \n#> 2     1 mpg          -0.101     0.0294   -3.43   0.00750 \n#> 3     1 qsec          0.0398    0.151     0.264  0.798   \n#> 4     1 gear         -0.0229    0.349    -0.0656 0.949   \n#> 5     0 (Intercept)   4.92      1.40      3.52   0.00309 \n#> 6     0 mpg          -0.192     0.0443   -4.33   0.000591\n#> 7     0 qsec          0.0919    0.0983    0.935  0.365   \n#> 8     0 gear          0.147     0.368     0.398  0.696\n```\n:::\n\n\nWhat if you want not just the `tidy()` output, but the `augment()` and `glance()` outputs as well, while still performing each regression only once? Since we're using list-columns, we can just fit the model once and use multiple list-columns to store the tidied, glanced and augmented outputs.\n\n\n::: {.cell layout-align=\"center\" hash='cache/unnamed-chunk-16_bfe852ae3caea34b424dc7d4ad083632'}\n\n```{.r .cell-code}\nregressions <- \n  mtcars %>%\n  nest(data = c(-am)) %>% \n  mutate(\n    fit = map(data, ~ lm(wt ~ mpg + qsec + gear, data = .x)),\n    tidied = map(fit, tidy),\n    glanced = map(fit, glance),\n    augmented = map(fit, augment)\n  )\n\nregressions %>% \n  select(tidied) %>% \n  unnest(tidied)\n#> # A tibble: 8 × 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)   4.28      3.46      1.24   0.247   \n#> 2 mpg          -0.101     0.0294   -3.43   0.00750 \n#> 3 qsec          0.0398    0.151     0.264  0.798   \n#> 4 gear         -0.0229    0.349    -0.0656 0.949   \n#> 5 (Intercept)   4.92      1.40      3.52   0.00309 \n#> 6 mpg          -0.192     0.0443   -4.33   0.000591\n#> 7 qsec          0.0919    0.0983    0.935  0.365   \n#> 8 gear          0.147     0.368     0.398  0.696\n\nregressions %>% \n  select(glanced) %>% \n  unnest(glanced)\n#> # A tibble: 2 × 12\n#>   r.squared adj.r.squared sigma statistic  p.value    df    logLik   AIC   BIC\n#>       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>     <dbl> <dbl> <dbl>\n#> 1     0.833         0.778 0.291     15.0  0.000759     3  -0.00580  10.0  12.8\n#> 2     0.625         0.550 0.522      8.32 0.00170      3 -12.4      34.7  39.4\n#> # ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nregressions %>% \n  select(augmented) %>% \n  unnest(augmented)\n#> # A tibble: 32 × 10\n#>       wt   mpg  qsec  gear .fitted  .resid  .hat .sigma  .cooksd .std.resid\n#>    <dbl> <dbl> <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>      <dbl>\n#>  1  2.62  21    16.5     4    2.73 -0.107  0.517  0.304 0.0744      -0.527 \n#>  2  2.88  21    17.0     4    2.75  0.126  0.273  0.304 0.0243       0.509 \n#>  3  2.32  22.8  18.6     4    2.63 -0.310  0.312  0.279 0.188       -1.29  \n#>  4  2.2   32.4  19.5     4    1.70  0.505  0.223  0.233 0.278        1.97  \n#>  5  1.62  30.4  18.5     4    1.86 -0.244  0.269  0.292 0.0889      -0.982 \n#>  6  1.84  33.9  19.9     4    1.56  0.274  0.286  0.286 0.125        1.12  \n#>  7  1.94  27.3  18.9     4    2.19 -0.253  0.151  0.293 0.0394      -0.942 \n#>  8  2.14  26    16.7     5    2.21 -0.0683 0.277  0.307 0.00732     -0.276 \n#>  9  1.51  30.4  16.9     5    1.77 -0.259  0.430  0.284 0.263       -1.18  \n#> 10  3.17  15.8  14.5     5    3.15  0.0193 0.292  0.308 0.000644     0.0789\n#> # ℹ 22 more rows\n```\n:::\n\n\nBy combining the estimates and p-values across all groups into the same tidy data frame (instead of a list of output model objects), a new class of analyses and visualizations becomes straightforward. This includes:\n\n- sorting by p-value or estimate to find the most significant terms across all tests,\n- p-value histograms, and\n- volcano plots comparing p-values to effect size estimates.\n\nIn each of these cases, we can easily filter, facet, or distinguish based on the `term` column. In short, this makes the tools of tidy data analysis available for the *results* of data analysis and models, not just the inputs.\n\n\n## Session information {#session-info}\n\n\n::: {.cell layout-align=\"center\" hash='cache/si_43a75b68dcc94565ba13180d7ad26a69'}\n\n```\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.3.0 (2023-04-21)\n#>  os       macOS Monterey 12.6\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/Los_Angeles\n#>  date     2023-05-25\n#>  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version date (UTC) lib source\n#>  broom      * 1.0.4   2023-03-11 [1] CRAN (R 4.3.0)\n#>  dials      * 1.2.0   2023-04-03 [1] CRAN (R 4.3.0)\n#>  dplyr      * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n#>  ggplot2    * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n#>  infer      * 1.0.4   2022-12-02 [1] CRAN (R 4.3.0)\n#>  parsnip    * 1.1.0   2023-04-12 [1] CRAN (R 4.3.0)\n#>  purrr      * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n#>  recipes    * 1.0.6   2023-04-25 [1] CRAN (R 4.3.0)\n#>  rlang        1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n#>  rsample    * 1.1.1   2022-12-07 [1] CRAN (R 4.3.0)\n#>  tibble     * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n#>  tidymodels * 1.1.0   2023-05-01 [1] CRAN (R 4.3.0)\n#>  tune       * 1.1.1   2023-04-11 [1] CRAN (R 4.3.0)\n#>  workflows  * 1.1.3   2023-02-22 [1] CRAN (R 4.3.0)\n#>  yardstick  * 1.2.0   2023-04-21 [1] CRAN (R 4.3.0)\n#> \n#>  [1] /Users/emilhvitfeldt/Library/R/arm64/4.3/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}