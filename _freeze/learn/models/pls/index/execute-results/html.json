{
  "hash": "ec7e110249609823915e0c48aba0fdfe",
  "result": {
    "markdown": "---\ntitle: \"Multivariate analysis using partial least squares\"\ncategories:\n  - pre-processing\n  - multivariate analysis\n  - partial least squares\ntype: learn-subsection\nweight: 6\ndescription: | \n  Build and fit a predictive model with more than one outcome.\ntoc: true\ntoc-depth: 2\ninclude-after-body: ../../../resources.html\n---\n\n\n\n\n\n\n\n## Introduction\n\nTo use code in this article,  you will need to install the following packages: modeldata, pls, and tidymodels.\n\n\"Multivariate analysis\" usually refers to multiple _outcomes_ being modeled, analyzed, and/or predicted. There are multivariate versions of many common statistical tools. For example, suppose there was a data set with columns `y1` and `y2` representing two outcomes to be predicted. The `lm()` function would look something like:\n\n\n::: {.cell layout-align=\"center\" hash='cache/lm_36d90c1aed59c0679cdfdde16dfcf574'}\n\n```{.r .cell-code}\nlm(cbind(y1, y2) ~ ., data = dat)\n```\n:::\n\n\nThis `cbind()` call is pretty awkward and is a consequence of how the traditional formula infrastructure works. The recipes package is a lot easier to work with! This article demonstrates how to model multiple outcomes.   \n\nThe data that we'll use has three outcomes. From `?modeldata::meats`:\n\n> \"These data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents.\n\n> \"For each meat sample the data consists of a 100 channel spectrum of absorbances and the contents of moisture (water), fat and protein. The absorbance is `-log10` of the transmittance measured by the spectrometer. The three contents, measured in percent, are determined by analytic chemistry.\"\n\nThe goal is to predict the proportion of the three substances using the chemistry test. There can often be a high degree of between-variable correlations in predictors, and that is certainly the case here. \n\nTo start, let's take the two data matrices (called `endpoints` and `absorp`) and bind them together in a data frame:\n\n\n::: {.cell layout-align=\"center\" hash='cache/data_7962e3e49e5e2bfe6e4626edd3a76067'}\n\n```{.r .cell-code}\nlibrary(modeldata)\ndata(meats)\n```\n:::\n\n\nThe three _outcomes_ have fairly high correlations also. \n\n## Preprocessing the data\n\nIf the outcomes can be predicted using a linear model, partial least squares (PLS) is an ideal method. PLS models the data as a function of a set of unobserved _latent_ variables that are derived in a manner similar to principal component analysis (PCA). \n\nPLS, unlike PCA, also incorporates the outcome data when creating the PLS components. Like PCA, it tries to maximize the variance of the predictors that are explained by the components but it also tries to simultaneously maximize the correlation between those components and the outcomes. In this way, PLS _chases_ variation of the predictors and outcomes. \n\nSince we are working with variances and covariances, we need to standardize the data. The recipe will center and scale all of the variables. \n\nMany base R functions that deal with multivariate outcomes using a formula require the use of `cbind()` on the left-hand side of the formula to work with the traditional formula methods. In tidymodels, recipes do not; the outcomes can be symbolically \"added\" together on the left-hand side:\n\n\n::: {.cell layout-align=\"center\" hash='cache/recipe_cadc0ff0922e4cf47f8b5ff54db7f0fa'}\n\n```{.r .cell-code}\nnorm_rec <- \n  recipe(water + fat + protein ~ ., data = meats) %>%\n  step_normalize(everything()) \n```\n:::\n\n\nBefore we can finalize the PLS model, the number of PLS components to retain must be determined. This can be done using performance metrics such as the root mean squared error. However, we can also calculate the proportion of variance explained by the components for the _predictors and each of the outcomes_. This allows an informed choice to be made based on the level of evidence that the situation requires. \n\nSince the data set isn't large, let's use resampling to measure these proportions. With ten repeats of 10-fold cross-validation, we build the PLS model on 90% of the data and evaluate on the heldout 10%. For each of the 100 models, we extract and save the proportions. \n\nThe folds can be created using the [rsample](https://rsample.tidymodels.org/) package and the recipe can be estimated for each resample using the [`prepper()`](https://rsample.tidymodels.org/reference/prepper.html) function: \n\n\n::: {.cell layout-align=\"center\" hash='cache/cv_92ea8083c6bb6e1a1890f5a0e59a5a0d'}\n\n```{.r .cell-code}\nset.seed(57343)\nfolds <- vfold_cv(meats, repeats = 10)\n\nfolds <- \n  folds %>%\n  mutate(recipes = map(splits, prepper, recipe = norm_rec))\n```\n:::\n\n\n## Partial least squares\n\nThe complicated parts for moving forward are:\n\n1. Formatting the predictors and outcomes into the format that the pls package requires, and\n2. Estimating the proportions. \n\nFor the first part, the standardized outcomes and predictors need to be formatted into two separate matrices. Since we used `retain = TRUE` when prepping the recipes, we can `bake()` with `new_data = NULl` to get the processed data back out. To save the data as a matrix, the option `composition = \"matrix\"` will avoid saving the data as tibbles and use the required format. \n\nThe pls package expects a simple formula to specify the model, but each side of the formula should _represent a matrix_. In other words, we need a data set with two columns where each column is a matrix. The secret to doing this is to \"protect\" the two matrices using `I()` when adding them to the data frame.\n\nThe calculation for the proportion of variance explained is straightforward for the predictors; the function `pls::explvar()` will compute that. For the outcomes, the process is more complicated. A ready-made function to compute these is not obvious but there is some code inside of the summary function to do the computation (see below). \n\nThe function `get_var_explained()` shown here will do all these computations and return a data frame with columns `components`, `source` (for the predictors, water, etc), and the `proportion` of variance that is explained by the components. \n\n\n\n::: {.cell layout-align=\"center\" hash='cache/var-explained_2acce73dfd57c3f85ec64c27afb58a06'}\n\n```{.r .cell-code}\nlibrary(pls)\n\nget_var_explained <- function(recipe, ...) {\n  \n  # Extract the predictors and outcomes into their own matrices\n  y_mat <- bake(recipe, new_data = NULL, composition = \"matrix\", all_outcomes())\n  x_mat <- bake(recipe, new_data = NULL, composition = \"matrix\", all_predictors())\n  \n  # The pls package prefers the data in a data frame where the outcome\n  # and predictors are in _matrices_. To make sure this is formatted\n  # properly, use the `I()` function to inhibit `data.frame()` from making\n  # all the individual columns. `pls_format` should have two columns.\n  pls_format <- data.frame(\n    endpoints = I(y_mat),\n    measurements = I(x_mat)\n  )\n  # Fit the model\n  mod <- plsr(endpoints ~ measurements, data = pls_format)\n  \n  # Get the proportion of the predictor variance that is explained\n  # by the model for different number of components. \n  xve <- explvar(mod)/100 \n\n  # To do the same for the outcome, it is more complex. This code \n  # was extracted from pls:::summary.mvr. \n  explained <- \n    drop(pls::R2(mod, estimate = \"train\", intercept = FALSE)$val) %>% \n    # transpose so that components are in rows\n    t() %>% \n    as_tibble() %>%\n    # Add the predictor proportions\n    mutate(predictors = cumsum(xve) %>% as.vector(),\n           components = seq_along(xve)) %>%\n    # Put into a tidy format that is tall\n    pivot_longer(\n      cols = c(-components),\n      names_to = \"source\",\n      values_to = \"proportion\"\n    )\n}\n```\n:::\n\n\nWe compute this data frame for each resample and save the results in the different columns. \n\n\n::: {.cell layout-align=\"center\" hash='cache/get-estimates_692f96aaee6ff253cc50aa0c09d2c872'}\n\n```{.r .cell-code}\nfolds <- \n  folds %>%\n  mutate(var = map(recipes, get_var_explained),\n         var = unname(var))\n```\n:::\n\n\nTo extract and aggregate these data, simple row binding can be used to stack the data vertically. Most of the action happens in the first 15 components so let's filter the data and compute the _average_ proportion.\n\n\n::: {.cell layout-align=\"center\" hash='cache/collapse-and-average_c90711694d17ab11dd03860f64ac6514'}\n\n```{.r .cell-code}\nvariance_data <- \n  bind_rows(folds[[\"var\"]]) %>%\n  filter(components <= 15) %>%\n  group_by(components, source) %>%\n  summarize(proportion = mean(proportion))\n#> `summarise()` has grouped output by 'components'. You can override\n#> using the `.groups` argument.\n```\n:::\n\n\nThe plot below shows that, if the protein measurement is important, you might require 10 or so components to achieve a good representation of that outcome. Note that the predictor variance is captured extremely well using a single component. This is due to the high degree of correlation in those data. \n\n\n::: {.cell layout-align=\"center\" hash='cache/plot_c869332810008a4caf85c4116e9906ea'}\n\n```{.r .cell-code}\nggplot(variance_data, aes(x = components, y = proportion, col = source)) + \n  geom_line(alpha = 0.5, size = 1.2) + \n  geom_point() \n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n```\n\n::: {.cell-output-display}\n![](figs/plot-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Session information {#session-info}\n\n\n::: {.cell layout-align=\"center\" hash='cache/si_43a75b68dcc94565ba13180d7ad26a69'}\n\n```\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.3.0 (2023-04-21)\n#>  os       macOS Monterey 12.6\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/Los_Angeles\n#>  date     2023-05-25\n#>  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version date (UTC) lib source\n#>  broom      * 1.0.4   2023-03-11 [1] CRAN (R 4.3.0)\n#>  dials      * 1.2.0   2023-04-03 [1] CRAN (R 4.3.0)\n#>  dplyr      * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n#>  ggplot2    * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n#>  infer      * 1.0.4   2022-12-02 [1] CRAN (R 4.3.0)\n#>  modeldata  * 1.1.0   2023-01-25 [1] CRAN (R 4.3.0)\n#>  parsnip    * 1.1.0   2023-04-12 [1] CRAN (R 4.3.0)\n#>  pls        * 2.8-1   2022-07-16 [1] CRAN (R 4.3.0)\n#>  purrr      * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n#>  recipes    * 1.0.6   2023-04-25 [1] CRAN (R 4.3.0)\n#>  rlang        1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n#>  rsample    * 1.1.1   2022-12-07 [1] CRAN (R 4.3.0)\n#>  tibble     * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n#>  tidymodels * 1.1.0   2023-05-01 [1] CRAN (R 4.3.0)\n#>  tune       * 1.1.1   2023-04-11 [1] CRAN (R 4.3.0)\n#>  workflows  * 1.1.3   2023-02-22 [1] CRAN (R 4.3.0)\n#>  yardstick  * 1.2.0   2023-04-21 [1] CRAN (R 4.3.0)\n#> \n#>  [1] /Users/emilhvitfeldt/Library/R/arm64/4.3/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}